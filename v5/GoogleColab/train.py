# ==========================================================
# ğŸ›¡ï¸ 0. ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶­æŒï¼ˆColabåˆ‡æ–­é˜²æ­¢ï¼‰
# ==========================================================
import IPython
import time
import os

display(IPython.display.Javascript('''
  function ConnectButton(){
    console.log("Auto-clicking connect button...");
    document.querySelector("#connect").click();
  }
  setInterval(ConnectButton, 60000);
'''))

print("âœ… [0] ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶­æŒã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
time.sleep(5)
print("=" * 60)


# ==========================================================
# ğŸ“¦ 1. å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# ==========================================================
print("ğŸ“¦ [1] ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
!pip install -q transformers datasets sentencepiece scikit-learn torch onnx onnxruntime optimum fugashi[unidic-lite] emoji pandas
!pip install -q --upgrade optimum
print("âœ… [1] ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã€‚")
print("=" * 60)


# ==========================================================
# ğŸ”— 1.5. Google Drive ãƒã‚¦ãƒ³ãƒˆ
# ==========================================================
print("ğŸ”— [1.5] Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...")
from google.colab import drive
drive.mount('/content/drive')
DRIVE_MODEL_DIR = "/content/drive/MyDrive/psan_comment_ai_drive"
os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)
print(f"âœ… ä¿å­˜å…ˆ: {DRIVE_MODEL_DIR}")
print("=" * 60)


# ==========================================================
# ğŸ§¹ 2. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†é–¢æ•°ï¼ˆæ—¥æœ¬èªç‰¹åŒ–ï¼‰
# ==========================================================
import re
import emoji
import unicodedata

def preprocess(text):
    if not isinstance(text, str) or not text.strip():
        return ""
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[wï½—]{3,}', 'ww', text)
    text = re.sub(r'[è‰]{3,}', 'è‰è‰', text)
    text = re.sub(r'[ï¼!]{2,}', 'ï¼', text)
    text = re.sub(r'[ï¼Ÿ?]{2,}', 'ï¼Ÿ', text)
    text = unicodedata.normalize('NFKC', text)
    return text.strip()


# ==========================================================
# ğŸ“Š 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ï¼ˆé¢¨ç´€ç¶­æŒç”¨ï¼‰
# ==========================================================
print("ğŸ“Š [3] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ä¸­...")
from datasets import load_dataset, concatenate_datasets, Dataset
import warnings
warnings.filterwarnings("ignore")

toxic_datasets = []
non_toxic_datasets = []

# ----------------------------
# (A) æ—¥æœ¬èªå°‚ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé¢¨ç´€ç¶­æŒç”¨ï¼‰
# ----------------------------

# A1. LLM-jp Toxicityï¼ˆä¸»ãƒ‡ãƒ¼ã‚¿ï¼‰- toxic (æœ€å¤§2000ä»¶)
print("   - LLM-jp Toxicity (toxic) ã‚’è¿½åŠ ä¸­...")
try:
    ds = load_dataset("p1atdev/LLM-jp-Toxicity-Dataset", split="train")
    ds_toxic_jp = ds.filter(lambda x: x["label"] == "toxic")
    ds_toxic_jp = ds_toxic_jp.shuffle(seed=42).select(range(min(2000, len(ds_toxic_jp))))
    ds_toxic_jp = ds_toxic_jp.map(
        lambda x: {"text": preprocess(x["text"]), "toxic": 1},
        remove_columns=["label"]
    )
    toxic_datasets.append(ds_toxic_jp)
    print(f"     â†’ {len(ds_toxic_jp)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# A1.1 LLM-jp Toxicityï¼ˆétoxicã®ã¿ä½¿ç”¨ï¼‰- non-toxic (æœ€å¤§2000ä»¶)
print("   - LLM-jp Toxicity (non-toxic) ã‚’è¿½åŠ ä¸­...")
try:
    ds = load_dataset("p1atdev/LLM-jp-Toxicity-Dataset", split="train")
    ds_non_toxic_jp = ds.filter(lambda x: x["label"] == "non-toxic")
    ds_non_toxic_jp = ds_non_toxic_jp.shuffle(seed=42).select(range(min(2000, len(ds_non_toxic_jp))))
    ds_non_toxic_jp = ds_non_toxic_jp.map(
        lambda x: {"text": preprocess(x["text"]), "toxic": 0},
        remove_columns=["label"]
    )
    non_toxic_datasets.append(ds_non_toxic_jp)
    print(f"     â†’ {len(ds_non_toxic_jp)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")


# A2. Attqa-Q JA - toxic (æœ€å¤§1000ä»¶)
print("   - Attqa-Q JA (toxic) ã‚’è¿½åŠ ä¸­...")
try:
    ds = load_dataset("ibm-research/AttaQ-JA", split="test")
    ds_toxic_attqa = ds.shuffle(seed=42).select(range(min(1000, len(ds))))
    ds_toxic_attqa = ds_toxic_attqa.map(
        lambda x: {"text": preprocess(x["input"]), "toxic": 1},
        remove_columns=['uid', 'label', 'input']
    )
    toxic_datasets.append(ds_toxic_attqa)
    print(f"     â†’ {len(ds_toxic_attqa)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# A3. textdetox/multilingual_toxicity_datasetï¼ˆæ—¥æœ¬èªéƒ¨åˆ†ï¼‰- toxic/non-toxic (å„æœ€å¤§500ä»¶)
print("   - textdetox (ja) ã‚’è¿½åŠ ä¸­...")
try:
    ds = load_dataset("textdetox/multilingual_toxicity_dataset", "default", split="ja")
    ds_toxic_detox = ds.filter(lambda x: x["toxic"] == True)
    ds_non_toxic_detox = ds.filter(lambda x: x["toxic"] == False)

    ds_toxic_detox = ds_toxic_detox.shuffle(seed=42).select(range(min(500, len(ds_toxic_detox))))
    ds_non_toxic_detox = ds_non_toxic_detox.shuffle(seed=42).select(range(min(500, len(ds_non_toxic_detox))))

    ds_toxic_detox = ds_toxic_detox.map(
        lambda x: {"text": preprocess(x["text"]), "toxic": 1},
        remove_columns=[c for c in ds_toxic_detox.column_names if c not in ["text", "toxic"]]
    )
    ds_non_toxic_detox = ds_non_toxic_detox.map(
        lambda x: {"text": preprocess(x["text"]), "toxic": 0},
        remove_columns=[c for c in ds_non_toxic_detox.column_names if c not in ["text", "toxic"]]
    )

    toxic_datasets.append(ds_toxic_detox)
    non_toxic_datasets.append(ds_non_toxic_detox)

    print(f"     â†’ toxic: {len(ds_toxic_detox)} ä»¶, non-toxic: {len(ds_non_toxic_detox)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# A4. æ‰‹å‹•é™°æ¹¿ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆè¿½åŠ ãƒ‡ãƒ¼ã‚¿ï¼‰- toxic (æœ€å¤§1500ä»¶)
print("   - æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ï¼ˆé™°æ¹¿ç³»ï¼‰ã‚’è¿½åŠ ä¸­...")
# é™°æ¹¿ã‚³ãƒ¡ãƒ³ãƒˆã‚’ Dataset.from_list ã§ä½œæˆ
stealth_examples = [
    {"text": "è¦‹ã¦ã‚‹ã ã‘ã§ä¸å¿«ã«ãªã‚‹é¡”ã—ã¦ã‚“ãª", "toxic": 1},
    {"text": "é ­æ‚ªã™ãã¦ä¼šè©±ã«ãªã‚‰ã‚“", "toxic": 1},
    {"text": "ãŠå‰ã®å­˜åœ¨ãŒå ´ã®ç©ºæ°—ã‚’æ‚ªãã—ã¦ã‚‹ã®æ°—ã¥ã„ã¦ï¼Ÿ", "toxic": 1},
    {"text": "ãªã‚“ã§ãã‚“ãªã«ç—›ã€…ã—ã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "ãŠå‰ã®ç™ºè¨€ã€å…¨éƒ¨è–„ã£ãºã‚‰ã„ã‚“ã ã‚ˆ", "toxic": 1},
    {"text": "ã»ã‚“ã¨ä½•ã‚‚å–ã‚ŠæŸ„ãªã„ã­", "toxic": 1},
    {"text": "ãŠå‰ãŒè€ƒãˆã¦ã‚‹ã“ã¨å…¨éƒ¨ã¤ã¾ã‚‰ã‚“", "toxic": 1},
    {"text": "è‡ªåˆ†ã®ç„¡èƒ½ã•ã«æ°—ã¥ã‹ãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "ã©ã‚“ãªè‚²ã¡æ–¹ã—ãŸã‚‰ãã‚“ãªæ€§æ ¼ã«ãªã‚‹ã‚“ã ã‚ˆ", "toxic": 1},
    {"text": "äººã¨é–¢ã‚ã‚‰ãªã„æ–¹ãŒã¿ã‚“ãªå¹¸ã›ã ã¨æ€ã†", "toxic": 1},
    {"text": "ä½•ã‚„ã£ã¦ã‚‚ä¸­é€”åŠç«¯ã ãª", "toxic": 1},
    {"text": "æ€§æ ¼è…ã£ã¦ã‚‹ã£ã¦è¨€ã‚ã‚ŒãŸã“ã¨ãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å‘¨ã‚Šã‹ã‚‰å«Œã‚ã‚Œã¦ã‚‹ã®æ°—ã¥ã„ã¦ãªã„ã®ã™ã”ã„ã­", "toxic": 1},
    {"text": "ãã®ä¾¡å€¤è¦³ã€æ™‚ä»£é…ã‚Œã£ã¦ãƒ¬ãƒ™ãƒ«ã˜ã‚ƒãªã„", "toxic": 1},
    {"text": "ã©ã†ã—ã¦ãã‚“ãªã«äººã‚’ä¸å¿«ã«ã•ã›ã‚‹å¤©æ‰ãªã®ï¼Ÿ", "toxic": 1},
    {"text": "è‡ªåˆ†ã ã‘ã¯ã¾ã¨ã‚‚ã ã¨æ€ã£ã¦ã‚‹ã‚¿ã‚¤ãƒ—ã ", "toxic": 1},
    {"text": "ãŠå‰ã®æ„è¦‹ã€èª°ã‚‚èˆˆå‘³ãªã„ã‚ˆ", "toxic": 1},
    {"text": "å–‹ã‚‹ãŸã³ã«çŸ¥èƒ½æŒ‡æ•°ãŒä¸‹ãŒã‚‹æ°—ãŒã™ã‚‹", "toxic": 1},
    {"text": "ã‚ˆããã‚Œã§ç”Ÿãã¦ã“ã‚ŒãŸãª", "toxic": 1},
    {"text": "ãŠå‰ã®å­˜åœ¨ãŒãƒã‚¤ã‚ºãªã‚“ã ã‚ˆ", "toxic": 1},
    {"text": "ãã®æ…‹åº¦ã€è¦‹ã¦ã¦å¯’æ°—ãŒã™ã‚‹", "toxic": 1},
    {"text": "åŠªåŠ›ã®æ–¹å‘æ€§é–“é•ã„ã™ã", "toxic": 1},
    {"text": "è©±ã›ã°è©±ã™ã»ã©ç—›ã€…ã—ã„", "toxic": 1},
    {"text": "å­¦ç¿’èƒ½åŠ›ã‚¼ãƒ­ã‹ã‚ˆ", "toxic": 1},
    {"text": "ã„ã¤ã‚‚è¨€ã£ã¦ã‚‹ã“ã¨ç©ºå›ã‚Šã—ã¦ã‚‹ã‚ˆã­", "toxic": 1},
    {"text": "ãŠå‰ã¨é–¢ã‚ã‚‹ã¨ä¸å¹¸ã«ãªã‚‹æ°—ãŒã™ã‚‹", "toxic": 1},
    {"text": "ç²¾ç¥å¹´é½¢ã©ã†ãªã£ã¦ã‚“ã®ï¼Ÿ", "toxic": 1},
    {"text": "é¡”ã‚‚ä¸­èº«ã‚‚é…·ã„ã£ã¦æ‰èƒ½ã ãª", "toxic": 1},
    {"text": "ãŠå‰ã®å­˜åœ¨ãŒãƒ–ãƒ©ãƒƒã‚¯ã‚¸ãƒ§ãƒ¼ã‚¯ã ã‚ˆ", "toxic": 1},
    {"text": "å›ã®ç¬‘é¡”ã€ãªã‚“ã‹å¼·å¼•ã«é ‘å¼µã£ã¦ã‚‹æ„ŸãŒä¼ã‚ã£ã¦ãã¦ç–²ã‚Œã‚‹ã‚", "toxic": 1},
    {"text": "åŠªåŠ›ã—ã¦ã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€æ­£ç›´ç„¡é§„ãªæ°—ãŒã—ãªãã‚‚ãªã„ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«ä¸€ç”Ÿæ‡¸å‘½ã‚„ã£ã¦ã‚‚ã€çµæœãŒå‡ºãªã„ã®ã£ã¦é‹ã˜ã‚ƒãªãã¦æ‰èƒ½ã®å•é¡Œã‹ã‚‚ã­", "toxic": 1},
    {"text": "æ„è¦‹ã‚’è¨€ã†ã®ã¯ã„ã„ã‘ã©ã€ã¿ã‚“ãªãŒèãä¾¡å€¤ãŒã‚ã‚‹ã¨æ€ã£ã¦ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‚ˆ", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã¯æ‚ªãã¯ãªã„ã‘ã©ã€ãªã„æ–¹ãŒã‚¹ãƒƒã‚­ãƒªã™ã‚‹ã‹ã‚‚", "toxic": 1},
    {"text": "é ‘å¼µã£ã¦ã‚‹å§¿ã¯ç«‹æ´¾ã ã‘ã©ã€çµå±€èª°ã«ã‚‚éŸ¿ã‹ãªã„ã‚“ã ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ãƒã‚¸ãƒ†ã‚£ãƒ–ã§ã„ã‚‹ã®ã€ç–²ã‚Œãªã„ï¼Ÿæœ¬éŸ³ã§ç”Ÿãã¦ã¿ãŸã‚‰ã©ã†ã‹ãª", "toxic": 1},
    {"text": "å›ã®è©±ã€èˆˆå‘³æ·±ãã¯ãªã„ã‘ã©ã€èããµã‚Šã—ã¦ã‚ã’ã‚‹ã‚ˆ", "toxic": 1},
    {"text": "è¦‹ãŸç›®ã¯æ™®é€šãªã‚“ã ã‘ã©ã€ä¸­èº«ãŒã¡ã‚‡ã£ã¨è–„ã£ãºã‚‰ã„æ°—ãŒã™ã‚‹", "toxic": 1},
    {"text": "å›ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã€æ–¬æ–°ã ã­ã€‚ã§ã‚‚ç¾å®Ÿã«å³ã•ãªã„ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«è‡ªå·±è‚¯å®šã—ã¦ã‚‹ã‘ã©ã€å¤–ã‹ã‚‰è¦‹ã‚‹ã¨ç—›ã€…ã—ã„ã‚ˆ", "toxic": 1},
    {"text": "å›ã®è¡Œå‹•ã€ã¿ã‚“ãªã®è¿·æƒ‘ã«ãªã£ã¦ã‚‹ã‹ã‚‚ã­ã€‚ã§ã‚‚è¨€ã‚ãªã„ã ã‘ã ã‚ˆ", "toxic": 1},
    {"text": "åŠªåŠ›ã¯èªã‚ã‚‹ã‘ã©ã€ãã‚ŒãŒç„¡æ„å‘³ã«çµ‚ã‚ã£ã¦ã‚‹ã®ã€æ°—ã¥ã„ã¦ãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®æ€§æ ¼ã€æ˜ã‚‹ã„ã®ã¯ã„ã„ã‘ã©ã€ã‚¦ã‚¶ã„æ™‚ã‚‚ã‚ã‚‹ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ç¤¾äº¤çš„ã§ã„ã‚‹ã®ã€ç–²ã‚Œãªã„ï¼Ÿæœ¬å½“ã¯å­¤ç«‹ã—ãŸæ–¹ãŒæ¥½ã‹ã‚‚", "toxic": 1},
    {"text": "å›ã®ä»•äº‹ã€ä¸å¯§ã«è¦‹ãˆã‚‹ã‘ã©ã€å®Ÿã¯é©å½“ã ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã‚’å°Šé‡ã™ã‚‹ã‚ˆã€‚ã§ã‚‚æ­£ç›´ã€ãã ã‚‰ãªã„ã¨æ€ã†", "toxic": 1},
    {"text": "å›ã®å¤¢ã€ç´ æ•µã ã‘ã©ã€ç¾å®Ÿçš„ã«ç„¡ç†ãã†", "toxic": 1},
    {"text": "ãã‚“ãªã«è‡ªä¿¡æº€ã€…ã§ã„ã‚‹ã®ã€è£ãŒã‚ã‚‹ã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®ç¬‘ã„æ–¹ã€ãªã‚“ã‹ä½œã‚Šç‰©ã¿ãŸã„ã ã‚ˆã­", "toxic": 1},
    {"text": "é ‘å¼µã£ã¦ã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€æˆæœãŒå‡ºãªã„ã®ã¯å›ã®ã›ã„ã ã‚ˆ", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€å‘¨ã‚Šã‚’æ´»æ°—ã¥ã‘ã¦ã‚‹ã‚ˆã†ã§å®Ÿã¯æ²ˆã‚ã¦ã‚‹ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«è©±ã™ã®ã€ã¿ã‚“ãªé€€å±ˆã—ã¦ã‚‹ã‹ã‚‚ã­", "toxic": 1},
    {"text": "å›ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€å€‹æ€§çš„ã ã‘ã©ã€æ™‚ä»£ã«åˆã‚ãªã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã¯è‡ªç”±ã ã‘ã©ã€å›ã®ã¯ãŸã ã®æ„šç—´ã«èã“ãˆã‚‹", "toxic": 1},
    {"text": "å›ã®åŠªåŠ›ã€è©•ä¾¡ã•ã‚Œã‚‹ã‹ã‚‚ã ã‘ã©ã€æ­£ç›´ç„¡è¦–ã•ã‚Œã‚‹ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«ãƒã‚¸ãƒ†ã‚£ãƒ–ã§ã„ã‚‹ã®ã€ã‚¹ãƒˆãƒ¬ã‚¹æºœã¾ã£ã¦ã‚‹ã‚“ã˜ã‚ƒãªã„ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€å„ªã—ã„ã‚ˆã†ã§åˆºã•ã‚‹ã‚ˆã­", "toxic": 1},
    {"text": "å›ã®äººç”Ÿã€å……å®Ÿã—ã¦ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã‘ã©ã€ç©ºè™šã ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ç¤¾äº¤çš„ã§ã„ã‚‹ã®ã€è£ã§å«Œã‚ã‚Œã¦ã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®ä»•äº‹ã€å®Œç’§ã«è¿‘ã„ã‘ã©ã€ãƒŸã‚¹ãŒå¤šã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã‚’è¨€ã†ã®ã¯ã„ã„ã‘ã©ã€èª°ã‚‚æœ¬æ°—ã§èã„ã¦ãªã„ã‚ˆ", "toxic": 1},
    {"text": "å›ã®ç¬‘é¡”ã€ã„ã„ã‚“ã ã‘ã©ã€ãªã‚“ã‹ä¸è‡ªç„¶ã ã‚ˆã­", "toxic": 1},
    {"text": "åŠªåŠ›ã—ã¦ã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€ãã‚ŒãŒç„¡é§„ã ã£ã¦æ°—ã¥ã‹ãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãƒ—ãƒ©ã‚¹ã«æ€ãˆã‚‹ã‘ã©ã€ãƒã‚¤ãƒŠã‚¹ã®æ–¹ãŒå¤šã„ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«è‡ªä¿¡ãŒã‚ã‚‹ã®ã€å¤±æ•—ã—ãŸæ™‚ãŒæ€–ã„ã‚ˆã­", "toxic": 1},
    {"text": "å›ã®è©±ã€é¢ç™½ã„ã‘ã©ã€èª°ã‚‚è¦šãˆã¦ãªã„ã‚ˆ", "toxic": 1},
    {"text": "å›ã®æ€§æ ¼ã€æ˜ã‚‹ã„ã®ã¯ã„ã„ã‘ã©ã€éå‰°ã ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«é ‘å¼µã£ã¦ã‚‹ã®ã€å ±ã‚ã‚Œãªã„ã‹ã‚‚ã­", "toxic": 1},
    {"text": "å›ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã€æ–°é®®ã ã‘ã©ã€å®Ÿè¡Œä¸èƒ½ã ã‚ˆ", "toxic": 1},
    {"text": "æ„è¦‹ã‚’å°Šé‡ã™ã‚‹ã‚ˆã€‚ã§ã‚‚å®Ÿéš›ã€ãã ã‚‰ãªã„ã¨æ€ã†", "toxic": 1},
    {"text": "å›ã®ç”Ÿæ´»ã€å®‰å®šã—ã¦ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã‘ã©ã€è„†ã„ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ç¤¾äº¤çš„ã§ã„ã‚‹ã®ã€å­¤ç‹¬ã‚’æ„Ÿã˜ã¦ã‚‹ã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®ä»•äº‹ã€æ­£ç¢ºã ã‘ã©ã€å‰µé€ æ€§ãŒãªã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã¯ã„ã„ã‘ã©ã€å›ã®ã¯ãŸã ã®ã‚¨ã‚´ã ã‚ˆã­", "toxic": 1},
    {"text": "å›ã®å¤¢ã€ç«‹æ´¾ã ã‘ã©ã€ç¾å®Ÿé€ƒé¿ã¿ãŸã„", "toxic": 1},
    {"text": "ãã‚“ãªã«ãƒã‚¸ãƒ†ã‚£ãƒ–ã§ã„ã‚‹ã®ã€è™šæ§‹ã®ä¸–ç•Œã«ç”Ÿãã¦ã‚‹ã‚“ã˜ã‚ƒãªã„ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€å„ªã—ã„ã‘ã©ã€è£ãŒã‚ã‚‹æ°—ãŒã™ã‚‹", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€å‘¨ã‚Šã‚’æ”¯ãˆã¦ã‚‹ã‚ˆã†ã§ã€è² æ‹…ã«ãªã£ã¦ã‚‹ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«è©±ã™ã®ã€ã¿ã‚“ãªæˆ‘æ…¢ã—ã¦ã‚‹ã‹ã‚‚ã­", "toxic": 1},
    {"text": "å›ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€ç‹¬ç‰¹ã ã‘ã©ã€å¤è‡­ã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã‚’è¨€ã†ã®ã¯è‡ªç”±ã ã‘ã©ã€å›ã®ã¯è¿·æƒ‘ã ã‚ˆ", "toxic": 1},
    {"text": "å›ã®åŠªåŠ›ã€èªã‚ã‚‰ã‚Œã‚‹ã‹ã‚‚ã‘ã©ã€ç„¡æ„å‘³ã«çµ‚ã‚ã£ã¦ã‚‹", "toxic": 1},
    {"text": "ãã‚“ãªã«è‡ªä¿¡æº€ã€…ã§ã„ã‚‹ã®ã€æŒ«æŠ˜ãŒæ¥ã‚‹ã‚ˆ", "toxic": 1},
    {"text": "å›ã®ç¬‘ã„æ–¹ã€æ¥½ã—ã„ã‘ã©ã€å¼·å¼•ã ã‚ˆã­", "toxic": 1},
    {"text": "é ‘å¼µã£ã¦ã‚‹ã®ã¯ã‚ã‹ã‚‹ã‘ã©ã€ãã‚ŒãŒç©ºå›ã‚Šã—ã¦ã‚‹", "toxic": 1},
    {"text": "å›ã®äººç”Ÿã€å……å®Ÿã—ã¦ã‚‹ã‘ã©ã€è™šã—ã„ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ç¤¾äº¤çš„ã§ã„ã‚‹ã®ã€å«Œã‚ã‚Œã¦ã‚‹ã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®ä»•äº‹ã€ä¸å¯§ã ã‘ã©ã€åŠ¹ç‡æ‚ªã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã¯ã„ã„ã‘ã©ã€å›ã®ã¯ãŸã ã®å¦„æƒ³ã ã‚ˆã­", "toxic": 1},
    {"text": "å›ã®å¤¢ã€ç´ æ™´ã‚‰ã—ã„ã‘ã©ã€å¶ã‚ãªã„ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«ãƒã‚¸ãƒ†ã‚£ãƒ–ã§ã„ã‚‹ã®ã€ã‚¹ãƒˆãƒ¬ã‚¹ãŒæºœã¾ã£ã¦ã‚‹ã‚“ã˜ã‚ƒãªã„ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€ã„ã„ã‘ã©ã€ä¿¡ã˜ã‚‰ã‚Œãªã„ã‚ˆã­", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãƒ—ãƒ©ã‚¹ã ã‘ã©ã€ãƒã‚¤ãƒŠã‚¹ãŒå¤šã„ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«è‡ªä¿¡ãŒã‚ã‚‹ã®ã€å¤±æ•—ã—ãŸã‚‰ã©ã†ã™ã‚“ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è©±ã€é¢ç™½ã„ã‘ã©ã€å¿˜ã‚Œå»ã‚‰ã‚Œã‚‹ã‚ˆ", "toxic": 1},
    {"text": "å›ã®æ€§æ ¼ã€æ˜ã‚‹ã„ã‘ã©ã€éå‰°ã§ã‚¦ã‚¶ã„", "toxic": 1},
    {"text": "ãã‚“ãªã«é ‘å¼µã£ã¦ã‚‹ã®ã€çµå±€ç„¡é§„ã‹ã‚‚ã­", "toxic": 1},
    {"text": "å›ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã€æ–°é®®ã ã‘ã©ã€é¦¬é¹¿ã’ã¦ã‚‹", "toxic": 1},
    {"text": "æ„è¦‹ã‚’å°Šé‡ã™ã‚‹ã‘ã©ã€å›ã®ã¯ãã ã‚‰ãªã„", "toxic": 1},
    {"text": "å›ã®ç”Ÿæ´»ã€å®‰å®šã—ã¦ã‚‹ã‘ã©ã€è„†ã„ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ç¤¾äº¤çš„ã§ã„ã‚‹ã®ã€å­¤ç‹¬ã‚’æ„Ÿã˜ã¦ã‚‹ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®ä»•äº‹ã€æ­£ç¢ºã ã‘ã©ã€ã¤ã¾ã‚‰ãªã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã¯ã‚¨ã‚´ã ã‚ˆ", "toxic": 1},
    {"text": "å›ã®å¤¢ã€é€ƒé¿ã ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ãƒã‚¸ãƒ†ã‚£ãƒ–ã§ã„ã‚‹ã®ã€è™šæ§‹ã˜ã‚ƒãªã„ã®ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€å„ªã—ã„ã‘ã©ã€åˆºã•ã‚‹ã‚ˆã­", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€è² æ‹…ã ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«è©±ã™ã®ã€é€€å±ˆã‹ã‚‚ã­", "toxic": 1},
    {"text": "å›ã®ã‚¹ã‚¿ã‚¤ãƒ«ã€å¤ã„ã‚ˆã­", "toxic": 1},
    {"text": "æ„è¦‹ã‚’è¨€ã†ã®ã¯è‡ªç”±ã ã‘ã©ã€è¿·æƒ‘ã ã‚ˆ", "toxic": 1},
    {"text": "å›ã®åŠªåŠ›ã€ç„¡æ„å‘³ã«çµ‚ã‚ã£ã¦ã‚‹ã‚ˆ", "toxic": 1},
    {"text": "ãã‚“ãªã«è‡ªä¿¡æº€ã€…ã§ã„ã‚‹ã®ã€æŒ«æŠ˜æ¥ã‚‹ã‚ˆ", "toxic": 1},
    {"text": "å›ã®ç¬‘ã„æ–¹ã€ä¸è‡ªç„¶ã ã‚ˆã­", "toxic": 1},
    {"text": "é ‘å¼µã£ã¦ã‚‹ã‘ã©ã€ç©ºå›ã‚Šã—ã¦ã‚‹", "toxic": 1},
    {"text": "å›ã®äººç”Ÿã€è™šã—ã„ã‚ˆã­", "toxic": 1},
    {"text": "ãã‚“ãªã«ç¤¾äº¤çš„ã§ã„ã‚‹ã®ã€å«Œã‚ã‚Œã¦ã‚‹ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®ä»•äº‹ã€åŠ¹ç‡æ‚ªã„ã‚ˆã­", "toxic": 1},
    {"text": "æ­»ã­æ¶ˆãˆã‚ã‚«ã‚¹ã‚´ãƒŸã‚¯ã‚º", "toxic": 1},
    {"text": "ãŠå‰ã¯æœ¬å½“ã«ç„¡èƒ½ã ãª", "toxic": 1},
    {"text": "ãŠå‰ã®å­˜åœ¨ã€ãŸã ã®é‚ªæ‚ªã ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ã®ãŒç›®çš„ã¿ãŸã„", "toxic": 1},
    {"text": "ç¬‘é¡”ãŒé‚ªæ‚ªã ", "toxic": 1},
    {"text": "ãŠå‰ã¯ãŸã ã®ã‚¯ã‚ºã ", "toxic": 1},
    {"text": "ã“ã‚“ãªã«ç„¡èƒ½ãªäººã€çã—ã„ã­", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€æœ¬å½“ã«é‚ªé­”ã ", "toxic": 1},
    {"text": "ä»–äººã®åŠªåŠ›ã‚’è»½è”‘ã™ã‚‹ã®ã¯ã€ã‚ãªãŸãŒç„¡èƒ½ãªè¨¼æ‹ ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€ã¾ã‚‹ã§æ¯’ã¿ãŸã„", "toxic": 1},
    {"text": "ãŠå‰ãŒå­˜åœ¨ã™ã‚‹ã ã‘ã§ã€å ´ãŒæ±šããªã‚‹", "toxic": 1},
    {"text": "äººã®å­˜åœ¨ã‚’å¦å®šã™ã‚‹ãªã‚“ã¦ã€ãã‚Œè‡ªä½“ãŒç„¡èƒ½", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ãŸã‚ã«ã‚ã‚‹ã®ï¼Ÿ", "toxic": 1},
    {"text": "ãŠå‰ã¯æœ¬å½“ã«å¿ƒãŒå†·ãŸã„ã­", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€å ´ã®ç©ºæ°—ã‚’æ‚ªãã™ã‚‹ã ã‘", "toxic": 1},
    {"text": "ã‚ãªãŸã¯ãŸã ã®ç„¡ä¾¡å€¤ãªå­˜åœ¨", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ãŒã€ç§ã®æ°—åˆ†ã‚’å£Šã™", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å­˜åœ¨ä¾¡å€¤ãŒãªã„", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®æ¥½ã—ã¿ï¼Ÿ", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€èª°ã‹ã®å¿ƒã‚’å£Šã™å­˜åœ¨", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€æœ¬å½“ã«ç—›ã„ã‚ˆ", "toxic": 1},
    {"text": "ãŠå‰ãŒã„ã‚‹ã ã‘ã§ã€ç§ã¯ä¸å¿«ã«ãªã‚‹", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®è² æ‹…", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€èª°ã‹ã®äººç”Ÿã‚’æ±šã™å­˜åœ¨", "toxic": 1},
    {"text": "ã‚ãªãŸã®è¨€è‘‰ã¯ã€ãŸã ã®ç½µå€’", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®é‚ªæ‚ªãªã‚¨ãƒãƒ«ã‚®ãƒ¼", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡æ„å‘³ãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®å¾—æ„æŠ€ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ã®ãŒç›®çš„", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å­˜åœ¨ä¾¡å€¤ã®ãªã„ã‚¯ã‚º", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å«Œæ‚ªã•ã›ã‚‹", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å¿ƒã®æ¯’", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€å ´ã‚’æ±šã™ã ã‘", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡ä¾¡å€¤ãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã®å­˜åœ¨ã‚’å¦å®šã™ã‚‹ã€ãã®è¡Œç‚ºã¯ç„¡èƒ½", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ã®ãŒç›®çš„", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡æ„å‘³ãªå­˜åœ¨", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€èª°ã‹ã‚’è‹¦ã—ã‚ã‚‹", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®é‚ªæ‚ªãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®ç¿’æ…£ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã®å¿ƒã‚’å£Šã™", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å­˜åœ¨ä¾¡å€¤ãŒãªã„", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®é‚ªæ‚ª", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®ç›®çš„ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ãŸã‚ã®æ­¦å™¨", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å¿ƒã®ç—…", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®è² æ‹…", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡ä¾¡å€¤ãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã®å­˜åœ¨ã‚’å¦å®šã™ã‚‹ã€ãã‚Œã¯ç„¡èƒ½", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ã®ãŒç›®çš„", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡æ„å‘³ãªå­˜åœ¨", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€èª°ã‹ã‚’è‹¦ã—ã‚ã‚‹", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®é‚ªæ‚ªãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®ç¿’æ…£ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã®å¿ƒã‚’å£Šã™", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å­˜åœ¨ä¾¡å€¤ãŒãªã„", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®é‚ªæ‚ª", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®ç›®çš„ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ãŸã‚ã®æ­¦å™¨", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å¿ƒã®ç—…", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®è² æ‹…", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡ä¾¡å€¤ãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã®å­˜åœ¨ã‚’å¦å®šã™ã‚‹ã€ãã‚Œã¯ç„¡èƒ½", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ã®ãŒç›®çš„", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡æ„å‘³ãªå­˜åœ¨", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€èª°ã‹ã‚’è‹¦ã—ã‚ã‚‹", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®é‚ªæ‚ªãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®ç¿’æ…£ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã®å¿ƒã‚’å£Šã™", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å­˜åœ¨ä¾¡å€¤ãŒãªã„", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®é‚ªæ‚ª", "toxic": 1},
    {"text": "ä»–äººã‚’å‚·ã¤ã‘ã‚‹ã®ãŒã€ã‚ãªãŸã®ç›®çš„ï¼Ÿ", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ãŸã‚ã®æ­¦å™¨", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®å¿ƒã®ç—…", "toxic": 1},
    {"text": "å›ã®å­˜åœ¨ã€ãŸã ã®è² æ‹…", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡ä¾¡å€¤ãªå­˜åœ¨", "toxic": 1},
    {"text": "ä»–äººã®å­˜åœ¨ã‚’å¦å®šã™ã‚‹ã€ãã‚Œã¯ç„¡èƒ½", "toxic": 1},
    {"text": "å›ã®è¨€è‘‰ã€èª°ã‹ã‚’å‚·ã¤ã‘ã‚‹ã®ãŒç›®çš„", "toxic": 1},
    {"text": "ãŠå‰ã¯ã€ãŸã ã®ç„¡æ„å‘³ãªå­˜åœ¨", "toxic": 1},
]

stealth_ds = Dataset.from_list(stealth_examples)
stealth_ds = stealth_ds.map(lambda x: {"text": preprocess(x["text"]), "toxic": x["toxic"]})
toxic_datasets.append(stealth_ds)
print(f"     â†’ {len(stealth_ds)} ä»¶")

# A5. inspection-ai/japanese-toxic-datasetï¼ˆGitHubã‹ã‚‰ç›´æ¥ãƒ­ãƒ¼ãƒ‰ï¼‰ - toxic/non-toxic
print("   - inspection-ai/japanese-toxic-dataset (GitHub) ã‚’è¿½åŠ ä¸­...")
try:
    # GitHubã®CSVãƒ•ã‚¡ã‚¤ãƒ«URLã‚’ä½¿ç”¨
    csv_url = "https://raw.githubusercontent.com/inspection-ai/japanese-toxic-dataset/main/data/subset.csv"
    # pandasã§CSVã‚’èª­ã¿è¾¼ã¿
    import pandas as pd
    df = pd.read_csv(csv_url)

    # æœ‰å®³åº¦ã‚¹ã‚³ã‚¢ã‚’åˆ¤å®š
    def map_toxicity_score(row):
        toxic_score = row['Toxic']
        very_toxic_score = row['Very Toxic']
        return 1 if (toxic_score > 0 or very_toxic_score > 0) else 0

    # DataFrameã«ãƒ©ãƒ™ãƒ«åˆ—ã‚’è¿½åŠ 
    df['toxic'] = df.apply(map_toxicity_score, axis=1)

    # Hugging Face Datasetå½¢å¼ã«å¤‰æ›
    ds = Dataset.from_pandas(df[['text', 'toxic']].copy())
    ds = ds.map(lambda x: {"text": preprocess(x["text"]), "toxic": int(x["toxic"])})

    toxic_ds_github = ds.filter(lambda x: x["toxic"] == 1)
    non_toxic_ds_github = ds.filter(lambda x: x["toxic"] == 0)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ  (é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ—¢å­˜ãƒªã‚¹ãƒˆã«è¿½åŠ )
    toxic_datasets.append(toxic_ds_github)
    non_toxic_datasets.append(non_toxic_ds_github)

    print(f"     â†’ toxic: {len(toxic_ds_github)} ä»¶, non-toxic: {len(non_toxic_ds_github)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")


# ----------------------------
# (B) å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥æœ¬èªæŠ½å‡ºï¼‰
# ----------------------------

# B1. toxi-text-3Mï¼ˆæ—¥æœ¬èªéƒ¨åˆ†ï¼‰- toxic/non-toxic (å„æœ€å¤§500ä»¶)
print("   - toxi-text-3M (ja) ã‚’è¿½åŠ ä¸­...")
try:
    big_ds = load_dataset("FredZhang7/toxi-text-3M", split="train")
    ja_ds = big_ds.filter(lambda x: x.get("lang", "") == "ja")

    ja_toxic = ja_ds.filter(lambda x: x["is_toxic"] == 1)
    ja_non_toxic = ja_ds.filter(lambda x: x["is_toxic"] == 0)

    ja_toxic = ja_toxic.shuffle(seed=42).select(range(min(500, len(ja_toxic))))
    ja_non_toxic = ja_non_toxic.shuffle(seed=42).select(range(min(500, len(ja_non_toxic))))


    ja_toxic = ja_toxic.map(
        lambda x: {"text": preprocess(x["text"]), "toxic": 1},
        remove_columns=["lang", "is_toxic"]
    )
    ja_non_toxic = ja_non_toxic.map(
        lambda x: {"text": preprocess(x["text"]), "toxic": 0},
        remove_columns=["lang", "is_toxic"]
    )

    toxic_datasets.append(ja_toxic)
    non_toxic_datasets.append(ja_non_toxic)

    print(f"     â†’ toxic: {len(ja_toxic)} ä»¶, non-toxic: {len(ja_non_toxic)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# B2. toxi-text-3Mï¼ˆè‹±èªéƒ¨åˆ†ï¼‰- toxic/non-toxic (å„æœ€å¤§250ä»¶)
print("   - toxi-text-3M (en) ã‚’è¿½åŠ ä¸­...")
try:
    big_ds = load_dataset("FredZhang7/toxi-text-3M", split="train")
    en_ds = big_ds.filter(lambda x: x.get("lang", "") == "en")

    en_toxic = en_ds.filter(lambda x: x["is_toxic"] == 1)
    en_non_toxic = en_ds.filter(lambda x: x["is_toxic"] == 0)

    en_toxic = en_toxic.shuffle(seed=42).select(range(min(250, len(en_toxic))))
    en_non_toxic = en_non_toxic.shuffle(seed=42).select(range(min(250, len(en_non_toxic))))

    en_toxic = en_toxic.map(
        lambda x: {"text": x["text"], "toxic": 1},
        remove_columns=["lang", "is_toxic"]
    )
    en_non_toxic = en_non_toxic.map(
        lambda x: {"text": x["text"], "toxic": 0},
        remove_columns=["lang", "is_toxic"]
    )

    toxic_datasets.append(en_toxic)
    non_toxic_datasets.append(en_non_toxic)

    print(f"     â†’ toxic: {len(en_toxic)} ä»¶, non-toxic: {len(en_non_toxic)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# B3. toxi-text-3Mï¼ˆä¸­å›½èªéƒ¨åˆ†ï¼‰- toxic/non-toxic (å„æœ€å¤§250ä»¶)
print("   - toxi-text-3M (zh) ã‚’è¿½åŠ ä¸­...")
try:
    big_ds = load_dataset("FredZhang7/toxi-text-3M", split="train")
    zh_ds = big_ds.filter(lambda x: x.get("lang", "") == "zh")

    zh_toxic = zh_ds.filter(lambda x: x["is_toxic"] == 1)
    zh_non_toxic = zh_ds.filter(lambda x: x["is_toxic"] == 0)

    zh_toxic = zh_toxic.shuffle(seed=42).select(range(min(250, len(zh_toxic))))
    zh_non_toxic = zh_non_toxic.shuffle(seed=42).select(range(min(250, len(zh_non_toxic))))

    zh_toxic = zh_toxic.map(
        lambda x: {"text": x["text"], "toxic": 1},
        remove_columns=["lang", "is_toxic"]
    )
    zh_non_toxic = zh_non_toxic.map(
        lambda x: {"text": x["text"], "toxic": 0},
        remove_columns=["lang", "is_toxic"]
    )

    toxic_datasets.append(zh_toxic)
    non_toxic_datasets.append(zh_non_toxic)

    print(f"     â†’ toxic: {len(zh_toxic)} ä»¶, non-toxic: {len(zh_non_toxic)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# B4. toxi-text-3Mï¼ˆéŸ“å›½èªéƒ¨åˆ†ï¼‰- toxic/non-toxic (å„æœ€å¤§250ä»¶)
print("   - toxi-text-3M (ko) ã‚’è¿½åŠ ä¸­...")
try:
    big_ds = load_dataset("FredZhang7/toxi-text-3M", split="train")
    ko_ds = big_ds.filter(lambda x: x.get("lang", "") == "ko")

    ko_toxic = ko_ds.filter(lambda x: x["is_toxic"] == 1)
    ko_non_toxic = ko_ds.filter(lambda x: x["is_toxic"] == 0)

    ko_toxic = ko_toxic.shuffle(seed=42).select(range(min(250, len(ko_toxic))))
    ko_non_toxic = ko_non_toxic.shuffle(seed=42).select(range(min(250, len(ko_non_toxic))))

    ko_toxic = ko_toxic.map(
        lambda x: {"text": x["text"], "toxic": 1},
        remove_columns=["lang", "is_toxic"]
    )
    ko_non_toxic = ko_non_toxic.map(
        lambda x: {"text": x["text"], "toxic": 0},
        remove_columns=["lang", "is_toxic"]
    )

    toxic_datasets.append(ko_toxic)
    non_toxic_datasets.append(ko_non_toxic)

    print(f"     â†’ toxic: {len(ko_toxic)} ä»¶, non-toxic: {len(ko_non_toxic)} ä»¶")
except Exception as e:
    print(f"     âŒ å¤±æ•—: {e}")

# ----------------------------
# çµ±åˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»é‡è¤‡å‰Šé™¤ãƒ»ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
# ----------------------------

# ===== æœ‰å®³ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆè¿½åŠ ï¼ï¼‰ =====
# Noneã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é™¤å¤–ã—ã¦ã‹ã‚‰çµåˆï¼ˆæœ‰å®³ï¼‰
valid_toxic_datasets = [ds for ds in toxic_datasets if ds is not None]
combined_toxic = concatenate_datasets(valid_toxic_datasets) if valid_toxic_datasets else None

if combined_toxic:
    print(f"   - çµåˆæœ‰å®³ãƒ‡ãƒ¼ã‚¿ (é‡è¤‡å‰Šé™¤å‰): {len(combined_toxic)} ä»¶")
    # pandasçµŒç”±ã§é‡è¤‡å‰Šé™¤ï¼ˆtextåˆ—ã§ï¼‰
    df_toxic = combined_toxic.to_pandas()
    df_toxic = df_toxic.drop_duplicates(subset=["text"])
    # ç©ºç™½ãƒ»Noneãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    df_toxic = df_toxic[df_toxic["text"].notna() & (df_toxic["text"].str.strip() != "")]
    combined_toxic = Dataset.from_pandas(df_toxic.reset_index(drop=True))
    print(f"   - çµåˆæœ‰å®³ãƒ‡ãƒ¼ã‚¿ (é‡è¤‡å‰Šé™¤ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ): {len(combined_toxic)} ä»¶")
else:
    print("   - æœ‰å®³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# Noneã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é™¤å¤–ã—ã¦ã‹ã‚‰çµåˆï¼ˆéæœ‰å®³ï¼‰
valid_non_toxic_datasets = [ds for ds in non_toxic_datasets if ds is not None]
combined_non_toxic = concatenate_datasets(valid_non_toxic_datasets) if valid_non_toxic_datasets else None

if combined_non_toxic:
    print(f"   - çµåˆéæœ‰å®³ãƒ‡ãƒ¼ã‚¿ (é‡è¤‡å‰Šé™¤å‰): {len(combined_non_toxic)} ä»¶")
    # pandasçµŒç”±ã§é‡è¤‡å‰Šé™¤ï¼ˆtextåˆ—ã§ï¼‰
    df_non_toxic = combined_non_toxic.to_pandas()
    df_non_toxic = df_non_toxic.drop_duplicates(subset=["text"])
    # ç©ºç™½ãƒ»Noneãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    df_non_toxic = df_non_toxic[df_non_toxic["text"].notna() & (df_non_toxic["text"].str.strip() != "")]
    combined_non_toxic = Dataset.from_pandas(df_non_toxic.reset_index(drop=True))
    print(f"   - çµåˆéæœ‰å®³ãƒ‡ãƒ¼ã‚¿ (é‡è¤‡å‰Šé™¤ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ): {len(combined_non_toxic)} ä»¶")
else:
    print("   - éæœ‰å®³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ãƒ‡ãƒ¼ã‚¿æ•°ã®èª¿æ•´ï¼ˆãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼‰
final_toxic_count = len(combined_toxic) if combined_toxic else 0
final_non_toxic_count = len(combined_non_toxic) if combined_non_toxic else 0

# ãƒ‡ãƒ¼ã‚¿æ•°ã®èª¿æ•´ï¼ˆãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼‰
final_toxic_count = len(combined_toxic) if combined_toxic else 0
final_non_toxic_count = len(combined_non_toxic) if combined_non_toxic else 0

print(f"\n   - æœ€çµ‚èª¿æ•´å‰ã®ãƒ‡ãƒ¼ã‚¿æ•°: æœ‰å®³={final_toxic_count}, éæœ‰å®³={final_non_toxic_count}")

# ã‚ˆã‚Šå°‘ãªã„ã‚¯ãƒ©ã‚¹ã®æ•°ã«åˆã‚ã›ã¦ã€å¤šã„æ–¹ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
min_count = min(final_toxic_count, final_non_toxic_count)
if combined_toxic and final_toxic_count > min_count:
    combined_toxic = combined_toxic.shuffle(seed=42).select(range(min_count))
if combined_non_toxic and final_non_toxic_count > min_count:
    combined_non_toxic = combined_non_toxic.shuffle(seed=42).select(range(min_count))

final_toxic_count_after_balance = len(combined_toxic) if combined_toxic else 0
final_non_toxic_count_after_balance = len(combined_non_toxic) if combined_non_toxic else 0

print(f"   - ãƒãƒ©ãƒ³ã‚¹èª¿æ•´å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°: æœ‰å®³={final_toxic_count_after_balance}, éæœ‰å®³={final_non_toxic_count_after_balance}")

# æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµåˆ
all_datasets = []
if combined_toxic:
    all_datasets.append(combined_toxic)
if combined_non_toxic:
    all_datasets.append(combined_non_toxic)

if not all_datasets:
     raise RuntimeError("âŒ å‡¦ç†å¾Œã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

combined = concatenate_datasets(all_datasets)


# ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¨åˆ†å‰²ï¼ˆè¨“ç·´:ãƒ†ã‚¹ãƒˆ = 9:1ï¼‰
combined = combined.shuffle(seed=42).train_test_split(test_size=0.1)

train_dataset = combined["train"]
test_dataset = combined["test"]

print(f"\nâœ… æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰å®Œäº†:")
print(f"   - è¨“ç·´: {len(train_dataset):,} ä»¶")
print(f"   - ãƒ†ã‚¹ãƒˆ: {len(test_dataset):,} ä»¶")
print("=" * 60)


# ==========================================================
# ğŸ·ï¸ 4. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ï¼ˆè»½é‡DistilBERTï¼‰
# ==========================================================
print("ğŸ·ï¸ [4] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’æº–å‚™ä¸­...")
MODEL_NAME = "distilbert/distilbert-base-multilingual-cased"
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=96,
    )

train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4)
test_dataset = test_dataset.map(tokenize, batched=True, num_proc=4)

# ãƒ©ãƒ™ãƒ«çµ±ä¸€ & ä¸è¦ã‚«ãƒ©ãƒ å‰Šé™¤
train_dataset = train_dataset.rename_column("toxic", "label")
test_dataset = test_dataset.rename_column("toxic", "label")

keep_cols = ["input_ids", "attention_mask", "label"]
train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in keep_cols])
test_dataset = test_dataset.remove_columns([c for c in test_dataset.column_names if c not in keep_cols])

train_dataset.set_format(type="torch", columns=keep_cols)
test_dataset.set_format(type="torch", columns=keep_cols)

print("âœ… [4] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶å‡¦ç†å®Œäº†ã€‚")
print("=" * 60)


# ==========================================================
# ğŸ§  5. ãƒ¢ãƒ‡ãƒ« & Trainer è¨­å®šï¼ˆè»½é‡ç‰ˆï¼‰
# ==========================================================
print("ğŸ§  [5] ãƒ¢ãƒ‡ãƒ«ã¨Trainerã‚’æº–å‚™ä¸­...")
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, recall_score

model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds),
        "recall": recall_score(labels, preds),
    }

training_args = TrainingArguments(
    output_dir="./comment_model",
    num_train_epochs=4,                # â† 4ã‚¨ãƒãƒƒã‚¯ã«å¤‰æ›´
    per_device_train_batch_size=32,    # â† CPUå¯¾å¿œã®ãŸã‚å¢—åŠ 
    per_device_eval_batch_size=64,
    save_total_limit=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=20,
    fp16=False,                        # â† CPUå¯¾å¿œã®ãŸã‚False
    load_best_model_at_end=True,
    metric_for_best_model="f1",        # â† F1é‡è¦–ï¼ˆé¢¨ç´€ç¶­æŒï¼‰
    greater_is_better=True,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("âœ… [5] æº–å‚™å®Œäº†ã€‚")
print("=" * 60)


# ==========================================================
# â³ 6. å­¦ç¿’ or ãƒ­ãƒ¼ãƒ‰
# ==========================================================
LOCAL_MODEL_DIR = "./psan_comment_ai"
CONFIG_PATH = os.path.join(LOCAL_MODEL_DIR, "config.json")
DRIVE_CONFIG_PATH = os.path.join(DRIVE_MODEL_DIR, "config.json")

if os.path.exists(DRIVE_CONFIG_PATH):
    print("ğŸ” Drive ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = AutoModelForSequenceClassification.from_pretrained(DRIVE_MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained(DRIVE_MODEL_DIR)
elif os.path.exists(CONFIG_PATH):
    print("ğŸ” ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
else:
    print("â³ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    trainer.train()
    print("âœ… å­¦ç¿’å®Œäº†ï¼")
    model.save_pretrained(LOCAL_MODEL_DIR)
    tokenizer.save_pretrained(LOCAL_MODEL_DIR)
    !cp -r {LOCAL_MODEL_DIR}/* {DRIVE_MODEL_DIR}/
    print(f"âœ… Google Drive ã«ä¿å­˜: {DRIVE_MODEL_DIR}")

print("=" * 60)


# ==========================================================
# âš¡ 7. ONNX å¤‰æ›ï¼ˆé«˜é€Ÿç‰ˆï¼‰
# ==========================================================
LOCAL_MODEL_DIR = "./psan_comment_ai"
os.makedirs(LOCAL_MODEL_DIR, exist_ok=True) # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã«ä½œæˆ
LOCAL_ONNX = os.path.join(LOCAL_MODEL_DIR, "model.onnx")
print(f"Local ONNX Path: {LOCAL_ONNX}") # ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”¨
if not os.path.exists(LOCAL_ONNX):
    print("âš¡ ONNXå¤‰æ›ä¸­...")
    import torch
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    dummy_ids = torch.randint(0, tokenizer.vocab_size, (1, 96), dtype=torch.long).to(device)
    dummy_mask = torch.ones((1, 96), dtype=torch.long).to(device)
    try:
        torch.onnx.export(
            model, (dummy_ids, dummy_mask), LOCAL_ONNX,
            input_names=["input_ids", "attention_mask"],
            output_names=["logits"],
            dynamic_axes={"input_ids": {0: "batch", 1: "sequence"},
                          "attention_mask": {0: "batch", 1: "sequence"},
                          "logits": {0: "batch"}},
            opset_version=14, do_constant_folding=True
        )
        print("âœ… ONNXå¤‰æ›å®Œäº†ã€‚")
    except Exception as e:
        print(f"âŒ ONNXå¤‰æ›å¤±æ•—: {e}")
        # å¤‰æ›ãŒå¤±æ•—ã—ãŸå ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’å‡ºåŠ›ã—ã¦çµ‚äº†
        raise e

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å¾Œã« ONNX ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Drive ã«ã‚³ãƒ”ãƒ¼
    !cp {LOCAL_ONNX} {os.path.join(DRIVE_MODEL_DIR, "model.onnx")}
    print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ã‚’ Drive ã«ã‚³ãƒ”ãƒ¼: {DRIVE_MODEL_DIR}")
else:
    print("ğŸ” ONNXãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
print("=" * 60)


# ==========================================================
# ğŸ§ª 8. æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆé«˜é€Ÿç‰ˆï¼‰
# ==========================================================
if os.path.exists(LOCAL_ONNX):
    print("ğŸ§ª ONNXæ¨è«–ãƒ†ã‚¹ãƒˆ...")
    import onnxruntime as ort
    import numpy as np

    # ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–
    session_options = ort.SessionOptions()
    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL

    # CUDAãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯CPUã‚’ä½¿ç”¨
    try:
        session = ort.InferenceSession(LOCAL_ONNX, sess_options=session_options, providers=['CUDAExecutionProvider'])
        print("âœ… CUDAæ¨è«–æœ‰åŠ¹")
    except:
        session = ort.InferenceSession(LOCAL_ONNX, sess_options=session_options)
        print("âœ… CPUæ¨è«–ä½¿ç”¨")

    def softmax(x): return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)

    def quick_predict(text):
        inputs = tokenizer(text, return_tensors="np", padding="max_length", max_length=96)
        logits = session.run(None, {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"]
        })[0]
        pred = np.argmax(logits, axis=-1)[0]
        prob = softmax(logits)[0][1]
        return "æœ‰å®³" if prob > 0.9 else "éæœ‰å®³", prob

    test_texts = [
        "ãƒã‚¸ã§å­˜åœ¨ä¾¡å€¤ãªã„ã‚ˆã­w",
        "æ­»ã­ã°ã„ã„ã®ã«",
        "ãŠå‰ã»ã‚“ã¨ä½¿ãˆãªã„ã‚ˆãªï¼ˆç¬‘ï¼‰",
        "ã“ã‚“ã«ã¡ã¯ï¼",
        "Have a nice day!",
        "å›ã®å­˜åœ¨ã€ãªã„æ–¹ãŒã‚¹ãƒƒã‚­ãƒªã™ã‚‹ã‹ã‚‚",
        "ãŠå‰ã¯æœ¬å½“ã«ç„¡èƒ½ã ãª",
        "æ­»ã­ã‚„",
        "ç¬‘é¡”ãŒé‚ªæ‚ªã ",
        "ã‚ãªãŸã¯ãŸã ã®ã‚¯ã‚ºã "
    ]

    import time
    start = time.time()
    for text in test_texts:
        result, prob = quick_predict(text)
        print(f"{'ğŸ‡¯ğŸ‡µ' if any(c in text for c in 'ã‚ã„ã†ãˆãŠ') else 'ğŸŒ'} ã€Œ{text}ã€ â†’ {result} (p={prob:.3f})")
    elapsed = time.time() - start
    print(f"âœ… æ¨è«–å®Œäº†: {elapsed:.2f}ç§’")
else:
    print("âš ï¸ ONNXãƒ¢ãƒ‡ãƒ«ãªã—ã€‚")

print("\nâœ¨ å…¨å·¥ç¨‹å®Œäº†ï¼")
