# ==========================================================
# ğŸ›¡ï¸ 0. ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ‡ã‚Œé˜²æ­¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ==========================================================
import IPython
from google.colab import output
import time
import os

display(IPython.display.Javascript('''
  function ConnectButton(){
    console.log("Connect button automatically clicked!");
    document.querySelector("#connect").click()
  }
  setInterval(ConnectButton, 60000);
'''))

print("âœ… [0] ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶­æŒã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
time.sleep(5)
print("==========================================================")


# ==========================================================
# ğŸ“¦ 1. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# ==========================================================
print("ğŸ“¦ [1] å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
!pip install -q transformers datasets sentencepiece scikit-learn torch onnx onnxruntime janome langdetect optimum fugashi[unidic-lite]
!pip install -q --upgrade optimum
print("âœ… [1] ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# ğŸ”— 1.5. Google Drive ãƒã‚¦ãƒ³ãƒˆ
# ==========================================================
print("ğŸ”— [1.5] Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...")
from google.colab import drive
drive.mount('/content/drive')
DRIVE_MODEL_DIR = "/content/drive/MyDrive/psan_comment_ai_drive"
os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)
print(f"âœ… Google Drive ãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¾ã—ãŸã€‚ä¿å­˜å…ˆ: {DRIVE_MODEL_DIR}")
print("==========================================================")


# ==========================================================
# ğŸ“Š 2,3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & æ—¥æœ¬èªãƒ»è‹±èªæŠ½å‡º & çµ±åˆ
# ==========================================================
print("ğŸ“Š [2, 3] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...")

from datasets import load_dataset

dataset = load_dataset("textdetox/multilingual_toxicity_dataset")
ja_data = dataset['ja']
en_data = dataset['en']

# çµ±åˆã—ã¦åˆ†å‰²
from datasets import concatenate_datasets
combined = concatenate_datasets([ja_data, en_data])
combined = combined.train_test_split(test_size=0.1, seed=42)

train_dataset = combined['train'].shuffle(seed=42)
test_dataset = combined['test'].shuffle(seed=42)

print(f"   - è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_dataset)}")
print(f"   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_dataset)}")
print("âœ… [2, 3] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# ğŸ·ï¸ 4. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶æº–å‚™ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
# ==========================================================
print("ğŸ·ï¸ [4] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶æº–å‚™ä¸­...")

MODEL_NAME = "cl-tohoku/bert-base-japanese"
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(batch):
    return tokenizer(
        batch['text'],
        padding='max_length',
        truncation=True,
        max_length=128,
        return_token_type_ids=True  # å¿…é ˆï¼
    )

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

def rename_label(example):
    example['label'] = int(example['toxic'])  # bool â†’ int
    return example

train_dataset = train_dataset.map(rename_label)
test_dataset = test_dataset.map(rename_label)

train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])

print("âœ… [4] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶å‡¦ç†å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# ğŸ§  5-7. ãƒ¢ãƒ‡ãƒ«å®šç¾© & å­¦ç¿’è¨­å®š & Trainer
# ==========================================================
print("ğŸ§  [5, 6, 7] ãƒ¢ãƒ‡ãƒ«ã¨Trainerã‚’æº–å‚™ä¸­...")

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    accuracy = (predictions == torch.tensor(labels)).float().mean().item()
    return {"accuracy": accuracy}

training_args = TrainingArguments(
    output_dir="./comment_model",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    save_total_limit=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=50,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none",  # wandbç„¡åŠ¹åŒ–
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("âœ… [5, 6, 7] ãƒ¢ãƒ‡ãƒ«ã¨Traineræº–å‚™å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# â³ 8. å­¦ç¿’ or ãƒ­ãƒ¼ãƒ‰
# ==========================================================
LOCAL_MODEL_DIR = "./psan_comment_ai"
CONFIG_PATH = os.path.join(LOCAL_MODEL_DIR, "config.json")
DRIVE_CONFIG_PATH = os.path.join(DRIVE_MODEL_DIR, "config.json")

if os.path.exists(DRIVE_CONFIG_PATH):
    print("ğŸ” Google Drive ã«å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)
    !cp -r {DRIVE_MODEL_DIR}/* {LOCAL_MODEL_DIR}/
    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
elif os.path.exists(CONFIG_PATH):
    print("ğŸ” Colab ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
else:
    print("â³ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆæ¨å®š 2ã€œ3æ™‚é–“ï¼‰...")
    trainer.train()
    print("âœ… å­¦ç¿’å®Œäº†ï¼")
    model.save_pretrained(LOCAL_MODEL_DIR)
    tokenizer.save_pretrained(LOCAL_MODEL_DIR)
    print(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜: {LOCAL_MODEL_DIR}")
    !cp -r {LOCAL_MODEL_DIR}/* {DRIVE_MODEL_DIR}/
    print(f"âœ… Google Drive ã«ã‚³ãƒ”ãƒ¼: {DRIVE_MODEL_DIR}")

print("==========================================================")


# ==========================================================
# âš¡ 10. æ‰‹å‹• ONNX å¤‰æ›ï¼ˆopset 14 å¯¾å¿œç‰ˆï¼‰
# ==========================================================
LOCAL_ONNX_MODEL_PATH = os.path.join(LOCAL_MODEL_DIR, "model.onnx")
DRIVE_ONNX_MODEL_PATH = os.path.join(DRIVE_MODEL_DIR, "model.onnx")

if os.path.exists(LOCAL_ONNX_MODEL_PATH):
    print("ğŸ” ONNXãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
else:
    print("âš¡ æ‰‹å‹•ã§ ONNX å¤‰æ›ã‚’å®Ÿè¡Œä¸­ï¼ˆopset 14ï¼‰...")

    model.eval()
    dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 128), dtype=torch.long)
    dummy_attention_mask = torch.ones((1, 128), dtype=torch.long)
    dummy_token_type_ids = torch.zeros((1, 128), dtype=torch.long)

    try:
        torch.onnx.export(
            model,
            (dummy_input_ids, dummy_attention_mask, dummy_token_type_ids),
            LOCAL_ONNX_MODEL_PATH,
            input_names=["input_ids", "attention_mask", "token_type_ids"],
            output_names=["logits"],
            dynamic_axes={
                "input_ids": {0: "batch", 1: "sequence"},
                "attention_mask": {0: "batch", 1: "sequence"},
                "token_type_ids": {0: "batch", 1: "sequence"},
                "logits": {0: "batch"},
            },
            opset_version=14,  # â† ã“ã“ã‚’ 14 ã«å¤‰æ›´ï¼
            do_constant_folding=True,
            export_params=True,
        )
        print(f"âœ… ONNX å¤‰æ›æˆåŠŸ â†’ {LOCAL_ONNX_MODEL_PATH}")
        !cp {LOCAL_ONNX_MODEL_PATH} {DRIVE_ONNX_MODEL_PATH}
        print(f"âœ… Google Drive ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ ONNX å¤‰æ›å¤±æ•—: {e}")
        print("ğŸ’¡ è©³ç´°: https://onnx.ai/ ã«ã‚ˆã‚‹ã¨ã€ONNX ã¯ Interoperability ã¨ Hardware Access ã‚’å®Ÿç¾ã—ã¾ã™ã€‚")

print("==========================================================")

# ==========================================================
# ğŸ§ª 11. ONNX æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰
# ==========================================================
if os.path.exists(LOCAL_ONNX_MODEL_PATH):
    print("ğŸ§ª ONNX æ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
    from onnxruntime import InferenceSession
    import numpy as np

    # softmax ã‚’æ­£ã—ãå®šç¾©
    def softmax(x, axis=-1):
        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

    test_text = "ã‚ãªãŸã¯æœ¬å½“ã«æœ€ä½ãªäººé–“ã§ã™ã­ã€‚"
    inputs = tokenizer(
        test_text,
        return_tensors="np",
        padding="max_length",
        max_length=128,
        return_token_type_ids=True
    )

    ort_session = InferenceSession(LOCAL_ONNX_MODEL_PATH)
    ort_out = ort_session.run(
        None,
        {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "token_type_ids": inputs["token_type_ids"],
        }
    )
    logits = ort_out[0]  # shape: (1, 2)
    pred = np.argmax(logits, axis=-1)[0]
    prob = softmax(logits, axis=-1)[0]  # â† ä¿®æ­£ç®‡æ‰€

    print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
    print(f"äºˆæ¸¬: {'æœ‰å®³' if pred == 1 else 'éæœ‰å®³'} (æœ‰å®³ç¢ºç‡: {prob[1]:.3f})")
else:
    print("âš ï¸ ONNXãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ¨è«–ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

print("\nâœ¨ å…¨å·¥ç¨‹å®Œäº†ï¼")
print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰: {LOCAL_MODEL_DIR}")
print(f"ONNXãƒ•ã‚¡ã‚¤ãƒ«: {LOCAL_ONNX_MODEL_PATH if os.path.exists(LOCAL_ONNX_MODEL_PATH) else 'ãªã—'}")
