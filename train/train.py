# -*- coding: utf-8 -*-
# Google Colab ã§å®Ÿè¡Œ

!pip install fugashi[unidic-lite] transformers datasets tokenizers torch onnx scikit-learn

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd, random, numpy as np, torch, os, json
from datetime import datetime
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
from sklearn.metrics import accuracy_score, f1_score
import re, unicodedata

# ä¹±æ•°å›ºå®šï¼ˆå†ç¾æ€§UPï¼‰
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ----------------------------
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ----------------------------
file_path = '/content/drive/MyDrive/Dataset/combined_toxicity_dataset.json'

try:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    df = pd.DataFrame(data)
    print("âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    print("ãƒ‡ãƒ¼ã‚¿æ•°:", len(df))
    print("ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
    print(df['labels'].value_counts())
except Exception as e:
    print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    exit()

# ----------------------------
# 2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
# ----------------------------
df = df.dropna(subset=['text'])
df = df[df['text'].str.len() > 0].reset_index(drop=True)

# ãƒ©ãƒ™ãƒ«åˆ¤å®šãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆ1=æœ‰å®³, 0=éæœ‰å®³ï¼‰
def is_toxic_label(v):
    try:
        return int(v) == 1
    except:
        if isinstance(v, str) and v.lower() in {'toxic','true','1'}:
            return True
    return False

df['is_toxic'] = df['labels'].apply(is_toxic_label)

# ----------------------------
# 3. æœ‰å®³ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼‹æ–‡å­—å´©ã—
# ----------------------------
toxic_df = df[df['is_toxic']].copy()
non_toxic_df = df[~df['is_toxic']].copy()

print(f"æŠ½å‡º: æœ‰å®³={len(toxic_df)} / éæœ‰å®³={len(non_toxic_df)}")

# æ­£è¦åŒ–é–¢æ•°
def normalize_text(text):
    text = unicodedata.normalize("NFKC", str(text))
    text = re.sub(r'[â—‹â—ã€‡â—¯*âœ±â˜…â˜†âšªï¸ãƒ»\u25CB\u25CF\u25A1\u25A0]', '', text)
    return text

# æ–‡å­—å´©ã—é–¢æ•°
def char_swap_variants(ch):
    mapping = {
        'ã”': ['5','ï½ºï¾','ã‚´'],
        'ã‚Š': ['ï¾˜','l','1','r'],
        'ã‚‰': ['ï¾—','ra'],
        'ã”ã‚Šã‚‰': ['ã”â—‹ã‚‰','ã”*ã‚‰','5ã‚Šã‚‰','ã‚´ãƒªãƒ©','ğŸµã‚‰'],
        'ã‚': ['ï½±','ã'],
        'ã„': ['ï½²','1','l'],
        'ãŠ': ['0','ã€‡','o'],
    }
    return mapping.get(ch, [])

def augment_text(text, prob=0.35):
    text = normalize_text(text)
    # é•·ã„å˜èªå„ªå…ˆ
    if 'ã”ã‚Šã‚‰' in text and random.random() < prob:
        return text.replace('ã”ã‚Šã‚‰', random.choice(['ã”â—‹ã‚‰','ã”*ã‚‰','5ã‚Šã‚‰','ã‚´ãƒªãƒ©','ğŸµã‚‰']))
    out = []
    for ch in text:
        if random.random() < prob:
            variants = char_swap_variants(ch)
            if variants:
                out.append(random.choice(variants))
            else:
                out.append(ch)
        else:
            out.append(ch)
    return ''.join(out)

# æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
def generate_augmented_toxics(toxic_df, multiplier=3, prob=0.35):
    aug_texts = []
    for _, row in toxic_df.iterrows():
        orig = row['text']
        for _ in range(multiplier):
            aug_texts.append({'text': augment_text(orig, prob), 'labels': row['labels']})
    return pd.DataFrame(aug_texts)

aug_df = generate_augmented_toxics(toxic_df, multiplier=3)
print(f"ç”Ÿæˆ: æ‹¡å¼µæœ‰å®³ãƒ‡ãƒ¼ã‚¿ = {len(aug_df)} ä»¶")

# å…ƒãƒ‡ãƒ¼ã‚¿ + æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆ
final_df = pd.concat([df[['text','labels']], aug_df], ignore_index=True)
final_df = final_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)
print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ•°: {len(final_df)}")

# ----------------------------
# 4. Datasetå¤‰æ›ï¼†åˆ†å‰²
# ----------------------------
dataset = Dataset.from_pandas(final_df).train_test_split(test_size=0.1, seed=seed)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# ----------------------------
# 5. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«
# ----------------------------
model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# ----------------------------
# 6. ãƒ¢ãƒ‡ãƒ«æº–å‚™ï¼†è©•ä¾¡é–¢æ•°
# ----------------------------
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds)
    }

# ----------------------------
# 7. å­¦ç¿’è¨­å®š
# ----------------------------
date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = f"/content/drive/MyDrive/toxicity_model_{date_str}"
os.makedirs(output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    eval_strategy="epoch",  # â† ã“ã“ã‚’ä¿®æ­£
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# ----------------------------
# 8. å­¦ç¿’
# ----------------------------
print("ğŸ”„ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
trainer.train()
print("âœ… å­¦ç¿’å®Œäº†")

# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {output_dir}")

# ----------------------------
# 9. ONNXå¤‰æ›
# ----------------------------
print("ğŸ”„ ONNXå¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()
dummy_input_ids = torch.randint(1, 1000, (1, 128)).to(device)
dummy_attention_mask = torch.ones((1, 128), dtype=torch.long).to(device)

torch.onnx.export(
    model,
    (dummy_input_ids, dummy_attention_mask),
    f"{output_dir}/toxic-bert-jp.onnx",
    export_params=True,
    opset_version=14,
    input_names=["input_ids", "attention_mask"],
    output_names=["logits"],
    dynamic_axes={
        "input_ids": {0: "batch", 1: "sequence"},
        "attention_mask": {0: "batch", 1: "sequence"},
        "logits": {0: "batch"},
    }
)
print(f"âœ… ONNXå¤‰æ›å®Œäº†: {output_dir}/toxic-bert-jp.onnx")
