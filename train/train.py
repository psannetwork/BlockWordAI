# -*- coding: utf-8 -*-
# Google Colab å®Ÿè¡Œç”¨
# æ—¥æœ¬èªæœ‰å®³ã‚³ãƒ¡ãƒ³ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆæ‹¡å¼µå¼·åŒ–ãƒ»éå­¦ç¿’æŠ‘åˆ¶ç‰ˆï¼‰

!pip install -q fugashi[unidic-lite] transformers datasets tokenizers torch scikit-learn nlpaug onnx

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd, random, numpy as np, torch, os, json, re, unicodedata
from datetime import datetime
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback
from sklearn.metrics import accuracy_score, f1_score
import nlpaug.augmenter.word as naw
from tqdm import tqdm

# =========================================================
# ä¹±æ•°å›ºå®š
# =========================================================
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# =========================================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =========================================================
file_path = '/content/drive/MyDrive/Dataset/combined_toxicity_dataset.json'
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)
df = pd.DataFrame(data)
print("âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†", len(df))

# =========================================================
# 2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
# =========================================================
df = df.dropna(subset=['text'])
df = df[df['text'].str.len() > 0].reset_index(drop=True)

def is_toxic_label(v):
    try: return int(v)==1
    except:
        if isinstance(v, str) and v.lower() in {'toxic','true','1'}: return True
    return False

df['is_toxic'] = df['labels'].apply(is_toxic_label)
toxic_df = df[df['is_toxic']].copy()
non_toxic_df = df[~df['is_toxic']].copy()
print(f"æŠ½å‡º: æœ‰å®³={len(toxic_df)} / éæœ‰å®³={len(non_toxic_df)}")

# =========================================================
# 3. ãƒã‚¤ã‚ºãƒ»è¨˜å·æ‹¡å¼µ
# =========================================================
def normalize_text(text):
    text = unicodedata.normalize("NFKC", str(text))
    text = re.sub(r'[â—‹â—ã€‡â—¯*âœ±â˜…â˜†âšªï¸ãƒ»\u25CB\u25CF\u25A1\u25A0]', '', text)
    return text

char_variants = {
    'ã‚': ['ï½±','ã','@','a'], 'ã„': ['ï½²','1','l','I','!','i'], 'ã†': ['ï½³','u'],
    'ãˆ': ['ï½´','e'], 'ãŠ': ['0','ã€‡','o','O'],
    'ã‹': ['ï½¶','k'], 'ã': ['ï½·','ki'], 'ã': ['ï½¸','ku'], 'ã‘': ['ï½¹','ke'], 'ã“': ['ï½º','ko'],
    'ã•': ['ï½»','s'], 'ã—': ['ï½¼','shi'], 'ã™': ['ï½½','su'], 'ã›': ['ï½¾','se'], 'ã': ['ï½¿','so'],
    'ãŸ': ['ï¾€','t'], 'ã¡': ['ï¾','chi'], 'ã¤': ['ï¾‚','ã£','tsu'], 'ã¦': ['ï¾ƒ','te'], 'ã¨': ['ï¾„','to'],
    'ãª': ['ï¾…','n'], 'ã«': ['ï¾†','ni'], 'ã¬': ['ï¾‡','nu'], 'ã­': ['ï¾ˆ','ne'], 'ã®': ['ï¾‰','no'],
    'ã¯': ['ï¾Š','h'], 'ã²': ['ï¾‹','hi'], 'ãµ': ['ï¾Œ','fu'], 'ã¸': ['ï¾','he'], 'ã»': ['ï¾','ho'],
    'ã¾': ['ï¾','m'], 'ã¿': ['ï¾','mi'], 'ã‚€': ['ï¾‘','mu'], 'ã‚': ['ï¾’','me'], 'ã‚‚': ['ï¾“','mo'],
    'ã‚„': ['ï¾”','ya'], 'ã‚†': ['ï¾•','yu'], 'ã‚ˆ': ['ï¾–','yo'],
    'ã‚‰': ['ï¾—','ra'], 'ã‚Š': ['ï¾˜','ri'], 'ã‚‹': ['ï¾™','ru'], 'ã‚Œ': ['ï¾š','re'], 'ã‚': ['ï¾›','ro'],
    'ã‚': ['ï¾œ','wa'], 'ã‚’': ['ï½¦','wo'], 'ã‚“': ['ï¾','n','nn'],
    ' ': ['ã€€','_','-'], 'ãƒ¼': ['ï½°','â€•','-'],
}

face_variants = ['â˜†','â˜…','â™ª','ğŸ’¢','âš¡','ğŸ”¥','ğŸ’€','ğŸ¤¬']

def augment_text(text, char_prob=0.35, face_prob=0.05):
    text = normalize_text(text)
    out = []
    for ch in text:
        if random.random() < char_prob:
            variants = char_variants.get(ch, [])
            out.append(random.choice(variants) if variants else ch)
        else:
            out.append(ch)
    if random.random() < face_prob:
        pos = random.randint(0, len(out))
        out.insert(pos, random.choice(face_variants))
    return ''.join(out)

# =========================================================
# 4. ãƒ•ãƒ¬ãƒ¼ã‚ºæ‹¡å¼µ
# =========================================================
auto_suffixes = [
    "ã§ã™ã­","ã ã­","ã˜ã‚ƒãªã„ã®","{}ã ã‚ˆ","ã‚ã‚“ãŸ{}","ã»ã‚“ã¨{}","ã‚‚ã†{}","ã•ã™ãŒ{}","ã­ãˆ{}",
    "{}ãªã®ã‹ãª","{}ã§ã—ã‚‡","{}ã ã‚ˆã­","{}ã ã£ã¦ã°","{}ã ã‚","{}ã‹ã‚‚ã­","{}ãªã‚“ã ã­","{}ãªã®ã ","{}ã ãª"
]

def expand_auto_phrases(text, multiplier=8):
    return [(s.format(text) if "{}" in s else text + s) for s in random.choices(auto_suffixes, k=multiplier)]

# =========================================================
# 5. åŒç¾©èªãƒ»å¤šæ§˜åŒ–æ‹¡å¼µï¼ˆnlpaugä½¿ç”¨ï¼‰
# =========================================================
synonym_aug = naw.SynonymAug(lang='jpn')

def generate_diverse_aug(df_subset, base_multiplier=3):
    aug_texts = []
    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset)):
        orig = row['text']
        label = row['labels']
        length = len(orig)

        # æ–‡é•·ãƒ»æœ‰å®³åº¦ã«å¿œã˜ã¦å€ç‡èª¿æ•´
        if row['is_toxic']:
            multiplier = base_multiplier + 2
        elif length <= 10:
            multiplier = base_multiplier + 1
        else:
            multiplier = base_multiplier

        for _ in range(multiplier):
            aug_choice = random.choice(['char', 'synonym'])
            if aug_choice == 'char':
                aug_text = augment_text(orig)
            else:
                try:
                    aug_text = synonym_aug.augment(orig)
                except:
                    aug_text = augment_text(orig)
            aug_texts.append({'text': aug_text, 'labels': label})

        # çŸ­æ–‡ã®ã¿èªå°¾ãƒ•ãƒ¬ãƒ¼ã‚ºè¿½åŠ 
        if len(orig) <= 10:
            for phrase in expand_auto_phrases(orig, multiplier=4):
                aug_texts.append({'text': phrase, 'labels': label})

    return pd.DataFrame(aug_texts)

# =========================================================
# 6. æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
# =========================================================
aug_toxic_df = generate_diverse_aug(toxic_df, base_multiplier=4)
aug_non_toxic_df = generate_diverse_aug(non_toxic_df, base_multiplier=3)

# çµåˆ + é‡è¤‡å‰Šé™¤
final_df = pd.concat([df[['text', 'labels']], aug_toxic_df, aug_non_toxic_df], ignore_index=True)
final_df = final_df.drop_duplicates(subset=['text']).sample(frac=1.0, random_state=seed).reset_index(drop=True)

print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ•°: {len(final_df)}")  # ç›®æ¨™: ç´„10ä¸‡ä»¶ï¼ˆãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ï¼‰

# =========================================================
# 7. Datasetå¤‰æ›ï¼†åˆ†å‰²
# =========================================================
dataset = Dataset.from_pandas(final_df).train_test_split(test_size=0.1, seed=seed)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# =========================================================
# 8. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼†ãƒ¢ãƒ‡ãƒ«
# =========================================================
model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.config.hidden_dropout_prob = 0.3  # dropoutå¢—åŠ ã§éå­¦ç¿’ç·©å’Œ

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {"accuracy": accuracy_score(labels, preds), "f1": f1_score(labels, preds)}

# =========================================================
# 9. å­¦ç¿’è¨­å®š
# =========================================================
date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = f"/content/drive/MyDrive/toxicity_model_{date_str}"
os.makedirs(output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.02,
    learning_rate=2e-5,
    logging_dir="./logs",
    logging_steps=100,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# =========================================================
# 10. å­¦ç¿’å®Ÿè¡Œ
# =========================================================
print("ğŸ”„ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
trainer.train()
print("âœ… å­¦ç¿’å®Œäº†")

# =========================================================
# 11. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
# =========================================================
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {output_dir}")

# =========================================================
# 12. ONNXå¤‰æ›
# =========================================================
print("ğŸ”„ ONNXå¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

dummy_input_ids = torch.randint(1, 1000, (1, 128)).to(device)
dummy_attention_mask = torch.ones((1, 128), dtype=torch.long).to(device)

torch.onnx.export(
    model,
    (dummy_input_ids, dummy_attention_mask),
    f"{output_dir}/toxic-bert-jp.onnx",
    export_params=True,
    opset_version=14,
    input_names=["input_ids", "attention_mask"],
    output_names=["logits"],
    dynamic_axes={
        "input_ids": {0: "batch", 1: "sequence"},
        "attention_mask": {0: "batch", 1: "sequence"},
        "logits": {0: "batch"}
    }
)
print(f"âœ… ONNXå¤‰æ›å®Œäº†: {output_dir}/toxic-bert-jp.onnx")
