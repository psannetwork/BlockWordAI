# -*- coding: utf-8 -*-
# Google Colab ã§å®Ÿè¡Œ

# 1. Google Drive æ¥ç¶š
from google.colab import drive
drive.mount('/content/drive')

# 2. å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# fugashi ã¨ã€fugashiãŒå‹•ä½œã™ã‚‹ãŸã‚ã®è¾æ›¸ unidic-lite ã‚’è¿½åŠ 
#!pip install onnxruntime transformers fugashi unidic-lite torch onnx

# 3. æœ€æ–°ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡º
import os
import glob

# æœ€ã‚‚æ–°ã—ã„æ—¥ä»˜ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
model_dirs = glob.glob('/content/drive/MyDrive/toxicity_model_*')
if not model_dirs:
    raise Exception("âŒ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

latest_dir = sorted(model_dirs, key=lambda x: os.path.getmtime(x))[-1]

print(f"âœ… æœ€æ–°ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {latest_dir}")

# 4. ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
model_file = f"{latest_dir}/toxic-bert-jp.onnx"
tokenizer_config_file = f"{latest_dir}/tokenizer_config.json"

# ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å¤‰æ›ã‚’å®Ÿè¡Œ
if not os.path.exists(model_file):
    print("ğŸ”„ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¤‰æ›ã‚’å®Ÿè¡Œã—ã¾ã™...")
    import torch
    from transformers import AutoModelForSequenceClassification

    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = AutoModelForSequenceClassification.from_pretrained(latest_dir)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device).eval()

    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®æº–å‚™
    dummy_input_ids = torch.randint(1, 1000, (1, 128)).to(device)
    dummy_attention_mask = torch.ones((1, 128), dtype=torch.long).to(device)

    # ONNXå¤‰æ›
    torch.onnx.export(
        model,
        (dummy_input_ids, dummy_attention_mask),
        model_file,
        export_params=True,
        opset_version=14,
        input_names=["input_ids", "attention_mask"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch", 1: "sequence"},
            "attention_mask": {0: "batch", 1: "sequence"},
            "logits": {0: "batch"},
        }
    )
    print(f"âœ… ONNXå¤‰æ›å®Œäº†: {model_file}")

if not os.path.exists(tokenizer_config_file):
    raise FileNotFoundError(f"âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®šãŒã‚ã‚Šã¾ã›ã‚“: {tokenizer_config_file}")

# 5. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆtransformersç‰ˆï¼‰
from transformers import AutoTokenizer
import onnxruntime as ort
import numpy as np

# AutoTokenizer ãŒé©åˆ‡ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹ï¼ˆã“ã®å ´åˆã¯ MecabTokenizerï¼‰ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã™
# è¾æ›¸ (unidic-lite) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
tokenizer = AutoTokenizer.from_pretrained(latest_dir)

session = ort.InferenceSession(model_file)

def predict_toxicity(text: str) -> float:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†ãŒ fugashi/unidic ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚ã‚Œã¾ã™
    enc = tokenizer(text[:200], truncation=True, padding=True, max_length=128)
    input_ids = np.array([enc["input_ids"]], dtype=np.int64)
    attention_mask = np.array([enc["attention_mask"]], dtype=np.int64)
    logits = session.run(None, {"input_ids": input_ids, "attention_mask": attention_mask})[0][0]
    probs = np.exp(logits) / np.sum(np.exp(logits))
    # ã‚¯ãƒ©ã‚¹ 1 ãŒæœ‰å®³ã§ã‚ã‚‹ç¢ºç‡ã¨ä»®å®š
    return float(probs[1])  # æœ‰å®³ç¢ºç‡ï¼ˆ0.0ï½1.0ï¼‰

# 6. å‹•ä½œãƒ†ã‚¹ãƒˆ
test_comments = [
    "ãˆã‚‰ã„ã­ã€œã€ã¾ãŸå¤±æ•—ã‹ã‚ˆ",
    "ç´ æ™´ã‚‰ã—ã„ã§ã™ã­ï¼",
    "ã‚¯ã‚½ã‚²ãƒ¼ã™ãã‚‹ã‚",
    "æ”¹å–„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„",
    "ç„¡ç†ã«ã—ã‚ã‚ˆã€ã©ã†ã›ã§ããªã„ãã›ã«",
    "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
    "ãƒã‚¸ç„¡ç†ã™ãã‚‹",
    "ãŒã‚“ã°ã£ã¦ãã ã•ã„",
    "æ­»ã­ã°ã„ã„ã®ã«",
    "ã¡ã‚‡ã£ã¨è€ƒãˆã™ãã˜ã‚ƒãªã„ï¼Ÿ",
    "ãƒ‘ãƒãƒ³ã‚«ã‚¹ãŒ",
    "ã²ã‚ˆã£ã¦ã‚‹ã­",
    "ãƒŸã‚¹ã£ãŸ",
    "ã–ãã“",
    "ã‚ˆã‚ww",
    "ãŠè…¹ã„ã£ã±ã„",
    "ç¤¾ç•œ",
    "äºˆæƒ³ãŒä»˜ãã¾ã›ã‚“ã­",
    "ã˜ã˜ã„",
    "ã‚¬ã‚­",
]

print("ğŸ” é¢¨ç´€ä¹±ã‚Œæ¤œå‡ºãƒ†ã‚¹ãƒˆ:")
for comment in test_comments:
    score = predict_toxicity(comment)
    print(f"ã‚³ãƒ¡ãƒ³ãƒˆ: {comment} â†’ ç¢ºç‡: {score:.3f}")
