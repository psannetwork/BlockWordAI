# ==========================================================
# ğŸ›¡ï¸ 0. ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶­æŒï¼ˆColabåˆ‡æ–­é˜²æ­¢ï¼‰
# ==========================================================
import IPython
import time
import os

display(IPython.display.Javascript('''
  function ConnectButton(){
    console.log("Auto-clicking connect button...");
    document.querySelector("#connect").click();
  }
  setInterval(ConnectButton, 60000);
'''))

print("âœ… [0] ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶­æŒã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
time.sleep(5)
print("==========================================================")


# ==========================================================
# ğŸ“¦ 1. å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# ==========================================================
print("ğŸ“¦ [1] ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
!pip install -q transformers datasets sentencepiece scikit-learn torch onnx onnxruntime optimum fugashi[unidic-lite] emoji
!pip install -q --upgrade optimum
print("âœ… [1] ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# ğŸ”— 1.5. Google Drive ãƒã‚¦ãƒ³ãƒˆ
# ==========================================================
print("ğŸ”— [1.5] Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...")
from google.colab import drive
drive.mount('/content/drive')
DRIVE_MODEL_DIR = "/content/drive/MyDrive/psan_comment_ai_drive"
os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)
print(f"âœ… ä¿å­˜å…ˆ: {DRIVE_MODEL_DIR}")
print("==========================================================")


# ==========================================================
# ğŸ“Š 2-3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ï¼ˆå¤šè¨€èªå¼·åŒ– + æ—¥æœ¬èªç‰¹åŒ–ï¼‰
# ==========================================================
print("ğŸ“Š [2-3] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ä¸­...")

from datasets import load_dataset, concatenate_datasets
import re
import emoji
import unicodedata

# æ—¥æœ¬èªå‰å‡¦ç†ï¼ˆæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«ã®ã¿é©ç”¨ï¼‰
def preprocess_ja(text):
    if not isinstance(text, str) or not text.strip():
        return ""
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[wï½—]{3,}', 'ww', text)
    text = re.sub(r'[è‰]{3,}', 'è‰è‰', text)
    text = re.sub(r'[ï¼!]{2,}', 'ï¼', text)
    text = re.sub(r'[ï¼Ÿ?]{2,}', 'ï¼Ÿ', text)
    text = unicodedata.normalize('NFKC', text)
    return text.strip()

# === è‹±èªãƒ‡ãƒ¼ã‚¿ï¼ˆJigsawï¼‰===
print("   - è‹±èªãƒ‡ãƒ¼ã‚¿ã€ŒJigsaw Toxic Commentsï¼ˆKaggleç‰ˆï¼‰ã€ã‚’è¿½åŠ ä¸­...")
try:
    jigsaw = load_dataset("jigsaw-toxic-comment-classification-challenge", split="train")
    jigsaw = jigsaw.map(lambda x: {
        "text": x["comment_text"],
        "toxic": int(any([
            x["toxic"],
            x["severe_toxic"],
            x["obscene"],
            x["threat"],
            x["insult"],
            x["identity_hate"]
        ]))
    })
    jigsaw = jigsaw.remove_columns([col for col in jigsaw.column_names if col not in ["text", "toxic"]])
    print("âœ… è‹±èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ è‹±èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    jigsaw = None

# === å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ï¼ˆJigsaw Multilingualï¼‰===
print("   - å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã€ŒJigsaw Multilingualã€ã‚’è¿½åŠ ä¸­...")
try:
    jigsaw_multi = load_dataset("jigsaw_multilingual_toxic_comment_classification", "all_languages", split="train")
    jigsaw_multi = jigsaw_multi.map(lambda x: {"text": x["comment_text"], "toxic": int(x["toxic"])})
    jigsaw_multi = jigsaw_multi.remove_columns([col for col in jigsaw_multi.column_names if col not in ["text", "toxic"]])
    print("âœ… å¤šè¨€èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ å¤šè¨€èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    jigsaw_multi = None

# === æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ ===
print("   - åŸºæœ¬æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¸­...")
try:
    dataset = load_dataset("textdetox/multilingual_toxicity_dataset")
    ja_data = dataset['ja']
    print("âœ… æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    ja_data = None

print("   - Attqa-Q JA ã‚’è¿½åŠ ä¸­...")
try:
    attaq = load_dataset("ibm-research/AttaQ-JA", split="test")
    attaq_ja = attaq.map(lambda x: {"text": x["input"], "toxic": 1}, remove_columns=['uid', 'label', 'input'])
    print("âœ… Attqa-Q JA èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ Attqa-Q JA èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    attaq_ja = None

print("   - LLM-jp Toxicity ã‚’è¿½åŠ ä¸­...")
try:
    toxic_llmjp = load_dataset("p1atdev/LLM-jp-Toxicity-Dataset", split="train")
    toxic_llmjp = toxic_llmjp.map(
        lambda x: {"text": x["text"], "toxic": 1 if x["label"] == "toxic" else 0},
        remove_columns=["label"]
    )
    print("âœ… LLM-jp Toxicity èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ LLM-jp Toxicity èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    toxic_llmjp = None

print("   - Japanese-Hate-Speech ã‚’è¿½åŠ ä¸­...")
try:
    ja_hate = load_dataset("shunk031/Japanese-Hate-Speech", split="train")
    ja_hate = ja_hate.map(lambda x: {"text": x["text"], "toxic": 1})
    print("âœ… Japanese-Hate-Speech èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ Japanese-Hate-Speech èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    ja_hate = None

print("   - Japanese-Toxic-Comments ã‚’è¿½åŠ ä¸­...")
try:
    ja_toxic = load_dataset("yamaguchi1214/Japanese-Toxic-Comments", split="train")
    ja_toxic = ja_toxic.rename_column("comment", "text")
    ja_toxic = ja_toxic.map(lambda x: {"toxic": 1})
    print("âœ… Japanese-Toxic-Comments èª­ã¿è¾¼ã¿æˆåŠŸ")
except Exception as e:
    print(f"âŒ Japanese-Toxic-Comments èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    ja_toxic = None

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ„ã¿ç«‹ã¦ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘ï¼‰
datasets_to_use = []
if jigsaw is not None:
    datasets_to_use.append(jigsaw)
if jigsaw_multi is not None:
    datasets_to_use.append(jigsaw_multi)
if ja_data is not None:
    datasets_to_use.append(ja_data)
if attaq_ja is not None:
    datasets_to_use.append(attaq_ja)
if toxic_llmjp is not None:
    datasets_to_use.append(toxic_llmjp)
if ja_hate is not None:
    datasets_to_use.append(ja_hate)
if ja_toxic is not None:
    datasets_to_use.append(ja_toxic)

if not datasets_to_use:
    raise ValueError("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã«å‰å‡¦ç†ã‚’é©ç”¨
ja_datasets = [ja_data, attaq_ja, toxic_llmjp, ja_hate, ja_toxic]
ja_datasets_processed = []
for ds in ja_datasets:
    if ds is not None:
        ds = ds.map(lambda x: {"text": preprocess_ja(x["text"]), "toxic": x["toxic"]})
        ja_datasets_processed.append(ds)
    else:
        ja_datasets_processed.append(None)

# ã‚µãƒ–ãƒ©ãƒ™ãƒ«ä»˜ãã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆLLM-jp ã®ã¿ï¼‰
def has_sublabel(example):
    if example['toxic'] != 1:
        return False
    return any(example.get(k, 0) == 1 for k in ['obscene', 'discriminatory', 'violent', 'illegal', 'sexual'])

if toxic_llmjp is not None:
    toxic_llmjp_proc = toxic_llmjp
    toxic_with_sub = toxic_llmjp_proc.filter(has_sublabel)
    toxic_llmjp_balanced = concatenate_datasets([toxic_llmjp_proc, toxic_with_sub])
else:
    toxic_llmjp_balanced = None

# æ—¥æœ¬èªæœ€çµ‚ãƒ‡ãƒ¼ã‚¿
ja_final_list = []
for ds in ja_datasets_processed:
    if ds is not None:
        ja_final_list.append(ds)

ja_final = None
if ja_final_list:
    ja_final = concatenate_datasets(ja_final_list)

# å…¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘ï¼‰
combined = concatenate_datasets(datasets_to_use)
if ja_final is not None:
    combined = concatenate_datasets([combined, ja_final])

combined = combined.filter(lambda x: len(x["text"].strip()) > 0)
combined = combined.train_test_split(test_size=0.1, seed=42)

train_dataset = combined['train'].shuffle(seed=42)
test_dataset = combined['test'].shuffle(seed=42)

print(f"   - è¨“ç·´: {len(train_dataset)} ä»¶")
print(f"   - ãƒ†ã‚¹ãƒˆ: {len(test_dataset)} ä»¶")
print("âœ… [2-3] ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# ğŸ·ï¸ 4. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ï¼ˆxlm-roberta-baseï¼š100è¨€èªå¯¾å¿œï¼‰
# ==========================================================
print("ğŸ·ï¸ [4] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’æº–å‚™ä¸­...")

MODEL_NAME = "xlm-roberta-base"  # â† å¤šè¨€èªå¯¾å¿œ + æ—¥æœ¬èªã‚‚OK
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)

def tokenize(batch):
    return tokenizer(
        batch['text'],
        padding='max_length',
        truncation=True,
        max_length=128,
    )

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# ãƒ©ãƒ™ãƒ«çµ±ä¸€
def rename_label(example):
    example['label'] = int(example['toxic'])
    return example

train_dataset = train_dataset.map(rename_label)
test_dataset = test_dataset.map(rename_label)

# ä¸è¦ã‚«ãƒ©ãƒ å‰Šé™¤
keep_cols = ['input_ids', 'attention_mask', 'label']
train_dataset = train_dataset.remove_columns(
    [col for col in train_dataset.column_names if col not in keep_cols]
)
test_dataset = test_dataset.remove_columns(
    [col for col in test_dataset.column_names if col not in keep_cols]
)

train_dataset.set_format(type='torch', columns=keep_cols)
test_dataset.set_format(type='torch', columns=keep_cols)

print("âœ… [4] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶å‡¦ç†å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# ğŸ§  5-7. ãƒ¢ãƒ‡ãƒ«å®šç¾© & å­¦ç¿’è¨­å®šï¼ˆRecallé‡è¦–ï¼‰
# ==========================================================
print("ğŸ§  [5-7] ãƒ¢ãƒ‡ãƒ«ã¨Trainerã‚’æº–å‚™ä¸­...")

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, recall_score

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, zero_division=0),
        "recall": recall_score(labels, predictions, zero_division=0),
    }

training_args = TrainingArguments(
    output_dir="./comment_model",
    num_train_epochs=3,
    per_device_train_batch_size=16,  # XLM-RoBERTa ã¯é‡ã‚
    per_device_eval_batch_size=32,
    save_total_limit=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=50,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="recall",
    greater_is_better=True,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("âœ… [5-7] æº–å‚™å®Œäº†ã€‚")
print("==========================================================")


# ==========================================================
# â³ 8. å­¦ç¿’ or ãƒ­ãƒ¼ãƒ‰
# ==========================================================
LOCAL_MODEL_DIR = "./psan_comment_ai"
CONFIG_PATH = os.path.join(LOCAL_MODEL_DIR, "config.json")
DRIVE_CONFIG_PATH = os.path.join(DRIVE_MODEL_DIR, "config.json")

if os.path.exists(DRIVE_CONFIG_PATH):
    print("ğŸ” Google Drive ã«å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)
    !cp -r {os.path.dirname(DRIVE_CONFIG_PATH)}/* {LOCAL_MODEL_DIR}/
    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
elif os.path.exists(CONFIG_PATH):
    print("ğŸ” ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
else:
    print("â³ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå¤šè¨€èª + æ—¥æœ¬èªå¼·åŒ–ï¼‰...")
    trainer.train()
    print("âœ… å­¦ç¿’å®Œäº†ï¼")
    model.save_pretrained(LOCAL_MODEL_DIR)
    tokenizer.save_pretrained(LOCAL_MODEL_DIR)
    print(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜: {LOCAL_MODEL_DIR}")
    !cp -r {LOCAL_MODEL_DIR}/* {DRIVE_MODEL_DIR}/
    print(f"âœ… Google Drive ã«ä¿å­˜: {DRIVE_MODEL_DIR}")

print("==========================================================")


# ==========================================================
# âš¡ 9. ONNX å¤‰æ›
# ==========================================================
LOCAL_ONNX_MODEL_PATH = os.path.join(LOCAL_MODEL_DIR, "model.onnx")
DRIVE_ONNX_MODEL_PATH = os.path.join(DRIVE_MODEL_DIR, "model.onnx")

if not os.path.exists(LOCAL_ONNX_MODEL_PATH):
    print("âš¡ ONNXå¤‰æ›ä¸­ï¼ˆopset 14ï¼‰...")
    import torch
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 128), dtype=torch.long).to(device)
    dummy_attention_mask = torch.ones((1, 128), dtype=torch.long).to(device)

    try:
        torch.onnx.export(
            model,
            (dummy_input_ids, dummy_attention_mask),
            LOCAL_ONNX_MODEL_PATH,
            input_names=["input_ids", "attention_mask"],
            output_names=["logits"],
            dynamic_axes={
                "input_ids": {0: "batch", 1: "sequence"},
                "attention_mask": {0: "batch", 1: "sequence"},
                "logits": {0: "batch"},
            },
            opset_version=14,
            do_constant_folding=True,
            export_params=True,
        )
        print(f"âœ… ONNXå¤‰æ›æˆåŠŸ â†’ {LOCAL_ONNX_MODEL_PATH}")
        !cp {LOCAL_ONNX_MODEL_PATH} {DRIVE_ONNX_MODEL_PATH}
        print(f"âœ… Google Drive ã«ä¿å­˜: {DRIVE_MODEL_DIR}")
    except Exception as e:
        print(f"âŒ å¤‰æ›å¤±æ•—: {e}")
else:
    print("ğŸ” ONNXãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")

print("==========================================================")


# ==========================================================
# âš¡ 9.5. INT8é‡å­åŒ–ï¼ˆCPUé«˜é€ŸåŒ–ï¼‰
# ==========================================================
QUANTIZED_DIR = os.path.join(LOCAL_MODEL_DIR, "quantized")
QUANTIZED_ONNX = os.path.join(QUANTIZED_DIR, "model_quantized.onnx")

if not os.path.exists(QUANTIZED_ONNX):
    print("ğŸ” INT8é‡å­åŒ–ä¸­ï¼ˆCPUé«˜é€ŸåŒ–ï¼‰...")
    from optimum.onnxruntime import ORTQuantizer
    quantizer = ORTQuantizer.from_pretrained(LOCAL_MODEL_DIR, file_name="model.onnx")
    quantizer.quantize(save_dir=QUANTIZED_DIR)
    print(f"âœ… é‡å­åŒ–å®Œäº† â†’ {QUANTIZED_DIR}")
else:
    print("âœ… é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚")

print("==========================================================")


# ==========================================================
# ğŸ§ª 10. å¤šè¨€èªæ¨è«–ãƒ†ã‚¹ãƒˆ
# ==========================================================
if os.path.exists(QUANTIZED_ONNX):
    print("ğŸ§ª é‡å­åŒ–ONNXã§å¤šè¨€èªæ¨è«–ãƒ†ã‚¹ãƒˆä¸­...")
    from onnxruntime import InferenceSession
    import numpy as np

    def softmax(x, axis=-1):
        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

    test_texts = [
        # æ—¥æœ¬èªï¼ˆé™°æ¹¿ï¼‰
        "ãƒã‚¸ã§å­˜åœ¨ä¾¡å€¤ãªã„ã‚ˆã­w",
        "æ­»ã­ã°ã„ã„ã®ã«",
        # è‹±èª
        "You are a worthless piece of shit.",
        "Go kill yourself!",
        # ç„¡å®³
        "ã“ã‚“ã«ã¡ã¯ï¼",
        "Have a nice day!",
    ]

    ort_session = InferenceSession(QUANTIZED_ONNX)
    for text in test_texts:
        inputs = tokenizer(
            text,
            return_tensors="np",
            padding="max_length",
            max_length=128,
        )
        ort_out = ort_session.run(
            None,
            {
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs["attention_mask"],
            }
        )
        logits = ort_out[0]
        pred = np.argmax(logits, axis=-1)[0]
        prob = softmax(logits, axis=-1)[0]
        lang = "ğŸ‡¯ğŸ‡µ" if any(c in text for c in "ã‚ã„ã†ãˆãŠã‹ããã‘ã“") else "ğŸŒ"
        print(f"{lang} ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        print(f"  â†’ äºˆæ¸¬: {'æœ‰å®³' if pred == 1 else 'éæœ‰å®³'} (æœ‰å®³ç¢ºç‡: {prob[1]:.3f})")
else:
    print("âš ï¸ é‡å­åŒ–ONNXãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

print("\nâœ¨ å…¨å·¥ç¨‹å®Œäº†ï¼")
print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰: {LOCAL_MODEL_DIR}")
print(f"é‡å­åŒ–ONNX: {QUANTIZED_ONNX if os.path.exists(QUANTIZED_ONNX) else 'ãªã—'}")
