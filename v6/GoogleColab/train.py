# -*- coding: utf-8 -*-
# Google Colab ã§å®Ÿè¡Œ

# 0. fugashi ã¨è¾æ›¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install fugashi[unidic-lite]

# 1. Google Drive æ¥ç¶š
from google.colab import drive
drive.mount('/content/drive')

# 2. å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install transformers datasets tokenizers torch onnx

# 3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
import pandas as pd

df = pd.read_json('/content/drive/MyDrive/Dataset/combined_toxicity_dataset.json')
print("âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print("ãƒ‡ãƒ¼ã‚¿æ•°:", len(df))
print("ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
print(df['labels'].value_counts())

# 4. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
df = df.dropna(subset=['text'])
df = df[df['text'].str.len() > 0]

# 5. Dataset ã«å¤‰æ›
from datasets import Dataset
dataset = Dataset.from_pandas(df)

# 6. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 7. æ—¥ä»˜ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
from datetime import datetime
import os

date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = f"/content/drive/MyDrive/toxicity_model_{date_str}"
os.makedirs(output_dir, exist_ok=True)

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
model_save_path = f"{output_dir}/pytorch_model.bin"

if os.path.exists(model_save_path):
    print("âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    model = AutoModelForSequenceClassification.from_pretrained(output_dir, num_labels=2)
else:
    print("ğŸ”„ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=100,
        eval_strategy="no",
        save_strategy="no",
        load_best_model_at_end=False,
        save_total_limit=1,
        gradient_checkpointing=False,
        report_to="none",  # wandbã‚’ç„¡åŠ¹åŒ–
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        eval_dataset=None,
        tokenizer=tokenizer,
    )

    trainer.train()

    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    model.save_pretrained(output_dir)
    print(f"âœ… å­¦ç¿’å®Œäº† & ãƒ¢ãƒ‡ãƒ«ã‚’ {output_dir} ã«ä¿å­˜ã—ã¾ã—ãŸ")

# 8. ONNXå¤‰æ›
print("ğŸ”„ ONNXå¤‰æ›ã‚’é–‹å§‹ã—ã¾ã™...")

# GPUãŒä½¿ãˆã‚‹ã‹ç¢ºèª
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚‚GPUã«é€ã‚‹
dummy_input_ids = torch.randint(1, 1000, (1, 128)).to(device)
dummy_attention_mask = torch.ones((1, 128), dtype=torch.long).to(device)

torch.onnx.export(
    model,
    (dummy_input_ids, dummy_attention_mask),
    f"{output_dir}/toxic-bert-jp.onnx",
    export_params=True,
    opset_version=14,
    input_names=["input_ids", "attention_mask"],
    output_names=["logits"],
    dynamic_axes={
        "input_ids": {0: "batch", 1: "sequence"},
        "attention_mask": {0: "batch", 1: "sequence"},
        "logits": {0: "batch"},
    }
)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿å­˜
tokenizer.save_pretrained(output_dir)

print(f"âœ… ONNXå¤‰æ›å®Œäº†")
print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_dir}")
print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«: {output_dir}/toxic-bert-jp.onnx, {output_dir}/tokenizer.json")
